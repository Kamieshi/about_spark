{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars ./app_data/postgresql-42.5.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[3]\")\n",
    "    .config(\"spark.jars\", \"./postgresql-42.5.0.jar\")\n",
    "    .appName(\"Driver_\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "session.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df = (session.\n",
    "      read.\n",
    "      format(\"jdbc\")\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\n",
    "      .option(\"user\", \"user\")\n",
    "      .option(\"password\", \"password\").option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", \"test\")\n",
    "      # .option(\"query\", \"SELECT * FROM test WHERE level='ERROR'\")\n",
    "      .load()\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_pk: integer (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- update_time: timestamp (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "Row(id_pk=1, level='ERROR', update_time=datetime.datetime(1970, 1, 1, 19, 36, 11), data='Data 1')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 19:44:07 ERROR FileOutputCommitter: Mkdirs failed to create file:/data_from_db/_temporary/0\n",
      "22/09/07 19:44:07 ERROR Executor: Exception in task 0.0 in stage 9.0 (TID 9)\n",
      "java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/07 19:44:07 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 9) (192.168.1.81 executor driver): java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/09/07 19:44:07 ERROR TaskSetManager: Task 0 in stage 9.0 failed 1 times; aborting job\n",
      "22/09/07 19:44:07 ERROR FileFormatWriter: Aborting job 0acb3816-660b-463a-ae64-1394eb53ccc7.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (192.168.1.81 executor driver): java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n",
      "\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n",
      "\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o109.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (192.168.1.81 executor driver): java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/data_from_db\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/SparkArticle/venv/lib/python3.8/site-packages/pyspark/sql/readwriter.py:1240\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1222\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m   1223\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[1;32m   1224\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1238\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[1;32m   1239\u001B[0m )\n\u001B[0;32m-> 1240\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/SparkArticle/venv/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/PycharmProjects/SparkArticle/venv/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    192\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[0;32m~/PycharmProjects/SparkArticle/venv/lib/python3.8/site-packages/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o109.csv.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:638)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9) (192.168.1.81 executor driver): java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\nCaused by: java.io.IOException: Mkdirs failed to create file:/data_from_db/_temporary/0/_temporary/attempt_202209071944074744373974544920044_0009_m_000000_9 (exists=false, cwd=file:/home/dmitryrusack/Work/about_spark/practic_3)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\n\tat org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.<init>(CsvOutputWriter.scala:38)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anon$1.newInstance(CSVFileFormat.scala:84)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:317)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:256)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.write.csv(\"/data_from_db\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:25:15 INFO DAGScheduler: Registering RDD 342 (showString at <unknown>:0) as input to shuffle 20\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Got map stage job 106 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Final stage: ShuffleMapStage 126 (showString at <unknown>:0)\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting ShuffleMapStage 126 (MapPartitionsRDD[342] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 33.0 KiB, free 434.2 MiB)\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 434.2 MiB)\n",
      "22/09/07 10:25:15 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on 192.168.1.81:39211 (size: 15.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:25:15 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 126 (MapPartitionsRDD[342] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 126) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:25:15 INFO Executor: Running task 0.0 in stage 126.0 (TID 126)\n",
      "22/09/07 10:25:15 INFO JDBCRDD: closed connection\n",
      "22/09/07 10:25:15 INFO Executor: Finished task 0.0 in stage 126.0 (TID 126). 2453 bytes result sent to driver\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 126) in 42 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:25:15 INFO DAGScheduler: ShuffleMapStage 126 (showString at <unknown>:0) finished in 0.046 s\n",
      "22/09/07 10:25:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:25:15 INFO DAGScheduler: running: Set()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:25:15 INFO ShufflePartitionsUtil: For shuffle(20), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/07 10:25:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/09/07 10:25:15 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Got job 107 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Final stage: ResultStage 128 (showString at <unknown>:0)\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 127)\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[345] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 34.2 KiB, free 434.1 MiB)\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.1 MiB)\n",
      "22/09/07 10:25:15 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on 192.168.1.81:39211 (size: 16.3 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:25:15 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[345] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 127) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:25:15 INFO Executor: Running task 0.0 in stage 128.0 (TID 127)\n",
      "22/09/07 10:25:15 INFO ShuffleBlockFetcherIterator: Getting 1 (216.0 B) non-empty blocks including 1 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:25:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/07 10:25:15 INFO Executor: Finished task 0.0 in stage 128.0 (TID 127). 3603 bytes result sent to driver\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 127) in 15 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:25:15 INFO DAGScheduler: ResultStage 128 (showString at <unknown>:0) finished in 0.024 s\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Job 107 finished: showString at <unknown>:0, took 0.030871 s\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Registering RDD 348 (getRowsToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 21\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Got map stage job 108 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Final stage: ShuffleMapStage 129 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting ShuffleMapStage 129 (MapPartitionsRDD[348] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 33.1 KiB, free 434.1 MiB)\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:25:15 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 192.168.1.81:39211 (size: 15.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:25:15 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 129 (MapPartitionsRDD[348] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 128) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:25:15 INFO Executor: Running task 0.0 in stage 129.0 (TID 128)\n",
      "22/09/07 10:25:15 INFO JDBCRDD: closed connection\n",
      "22/09/07 10:25:15 INFO Executor: Finished task 0.0 in stage 129.0 (TID 128). 2453 bytes result sent to driver\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 128) in 35 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:25:15 INFO DAGScheduler: ShuffleMapStage 129 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.047 s\n",
      "22/09/07 10:25:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:25:15 INFO DAGScheduler: running: Set()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:25:15 INFO ShufflePartitionsUtil: For shuffle(21), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/07 10:25:15 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/09/07 10:25:15 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Got job 109 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Final stage: ResultStage 131 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 130)\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting ResultStage 131 (MapPartitionsRDD[351] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 34.2 KiB, free 434.0 MiB)\n",
      "22/09/07 10:25:15 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.0 MiB)\n",
      "22/09/07 10:25:15 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 192.168.1.81:39211 (size: 16.3 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:25:15 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 131 (MapPartitionsRDD[351] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 129) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:25:15 INFO Executor: Running task 0.0 in stage 131.0 (TID 129)\n",
      "22/09/07 10:25:15 INFO ShuffleBlockFetcherIterator: Getting 1 (216.0 B) non-empty blocks including 1 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:25:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "22/09/07 10:25:15 INFO Executor: Finished task 0.0 in stage 131.0 (TID 129). 3603 bytes result sent to driver\n",
      "22/09/07 10:25:15 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 129) in 11 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:25:15 INFO DAGScheduler: ResultStage 131 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:25:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 131: Stage finished\n",
      "22/09/07 10:25:15 INFO DAGScheduler: Job 109 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.019739 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-------+-----+\n|  level|count|\n+-------+-----+\n|   INFO| 1000|\n|  ERROR| 1000|\n|WARNING| 1000|\n+-------+-----+",
      "text/html": "<table border='1'>\n<tr><th>level</th><th>count</th></tr>\n<tr><td>INFO</td><td>1000</td></tr>\n<tr><td>ERROR</td><td>1000</td></tr>\n<tr><td>WARNING</td><td>1000</td></tr>\n</table>\n"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(F.col(\"level\")).count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:25:27 INFO DAGScheduler: Registering RDD 357 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 22\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Got map stage job 111 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Final stage: ShuffleMapStage 133 (count at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Submitting ShuffleMapStage 133 (MapPartitionsRDD[357] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:25:27 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 13.4 KiB, free 434.0 MiB)\n",
      "22/09/07 10:25:27 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.0 MiB)\n",
      "22/09/07 10:25:27 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on 192.168.1.81:39211 (size: 7.0 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:25:27 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 133 (MapPartitionsRDD[357] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:25:27 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:25:27 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 131) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:25:27 INFO Executor: Running task 0.0 in stage 133.0 (TID 131)\n",
      "22/09/07 10:25:27 INFO BlockManagerInfo: Removed broadcast_104_piece0 on 192.168.1.81:39211 in memory (size: 5.5 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:25:27 INFO JDBCRDD: closed connection\n",
      "22/09/07 10:25:27 INFO Executor: Finished task 0.0 in stage 133.0 (TID 131). 1881 bytes result sent to driver\n",
      "22/09/07 10:25:27 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 131) in 21 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:25:27 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:25:27 INFO DAGScheduler: ShuffleMapStage 133 (count at NativeMethodAccessorImpl.java:0) finished in 0.047 s\n",
      "22/09/07 10:25:27 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:25:27 INFO DAGScheduler: running: Set()\n",
      "22/09/07 10:25:27 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:25:27 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:25:27 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Got job 112 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Final stage: ResultStage 135 (count at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 134)\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[360] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:25:27 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 11.1 KiB, free 434.0 MiB)\n",
      "22/09/07 10:25:27 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.0 MiB)\n",
      "22/09/07 10:25:27 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on 192.168.1.81:39211 (size: 5.5 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:25:27 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[360] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:25:27 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:25:27 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 132) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:25:27 INFO Executor: Running task 0.0 in stage 135.0 (TID 132)\n",
      "22/09/07 10:25:27 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:25:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/07 10:25:27 INFO Executor: Finished task 0.0 in stage 135.0 (TID 132). 2656 bytes result sent to driver\n",
      "22/09/07 10:25:27 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 132) in 19 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:25:27 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:25:27 INFO DAGScheduler: ResultStage 135 (count at NativeMethodAccessorImpl.java:0) finished in 0.023 s\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Job 112 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:25:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished\n",
      "22/09/07 10:25:27 INFO DAGScheduler: Job 112 finished: count at NativeMethodAccessorImpl.java:0, took 0.024865 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"level\") == \"ERROR\").count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitryrusack/PycharmProjects/SparkArticle/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"temp_table\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:27:36 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Got job 121 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Final stage: ResultStage 145 (showString at <unknown>:0)\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Submitting ResultStage 145 (MapPartitionsRDD[387] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:27:36 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 12.6 KiB, free 434.1 MiB)\n",
      "22/09/07 10:27:36 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:27:36 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:27:36 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 145 (MapPartitionsRDD[387] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:27:36 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:27:36 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 141) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:27:36 INFO Executor: Running task 0.0 in stage 145.0 (TID 141)\n",
      "22/09/07 10:27:36 INFO JDBCRDD: closed connection\n",
      "22/09/07 10:27:36 INFO Executor: Finished task 0.0 in stage 145.0 (TID 141). 1782 bytes result sent to driver\n",
      "22/09/07 10:27:36 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 141) in 41 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:27:36 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:27:36 INFO DAGScheduler: ResultStage 145 (showString at <unknown>:0) finished in 0.045 s\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Job 121 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:27:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 145: Stage finished\n",
      "22/09/07 10:27:36 INFO DAGScheduler: Job 121 finished: showString at <unknown>:0, took 0.046490 s\n",
      "+-----+-----+-------------------+-------+\n",
      "|id_pk|level|        update_time|   data|\n",
      "+-----+-----+-------------------+-------+\n",
      "|    1|ERROR|1970-01-01 10:13:54| Data 1|\n",
      "|    2|ERROR|1970-01-01 10:13:54| Data 2|\n",
      "|    3|ERROR|1970-01-01 10:13:54| Data 3|\n",
      "|    4|ERROR|1970-01-01 10:13:54| Data 4|\n",
      "|    5|ERROR|1970-01-01 10:13:54| Data 5|\n",
      "|    6|ERROR|1970-01-01 10:13:54| Data 6|\n",
      "|    7|ERROR|1970-01-01 10:13:54| Data 7|\n",
      "|    8|ERROR|1970-01-01 10:13:54| Data 8|\n",
      "|    9|ERROR|1970-01-01 10:13:54| Data 9|\n",
      "|   10|ERROR|1970-01-01 10:13:54|Data 10|\n",
      "|   11|ERROR|1970-01-01 10:13:54|Data 11|\n",
      "|   12|ERROR|1970-01-01 10:13:54|Data 12|\n",
      "|   13|ERROR|1970-01-01 10:13:54|Data 13|\n",
      "|   14|ERROR|1970-01-01 10:13:54|Data 14|\n",
      "|   15|ERROR|1970-01-01 10:13:54|Data 15|\n",
      "|   16|ERROR|1970-01-01 10:13:54|Data 16|\n",
      "|   17|ERROR|1970-01-01 10:13:54|Data 17|\n",
      "|   18|ERROR|1970-01-01 10:13:54|Data 18|\n",
      "|   19|ERROR|1970-01-01 10:13:54|Data 19|\n",
      "|   20|ERROR|1970-01-01 10:13:54|Data 20|\n",
      "+-----+-----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"SELECT * FROM temp_table WHERE level='ERROR'\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:28:17 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Got job 123 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Final stage: ResultStage 147 (showString at <unknown>:0)\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Submitting ResultStage 147 (MapPartitionsRDD[393] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:28:17 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 12.7 KiB, free 434.3 MiB)\n",
      "22/09/07 10:28:17 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.3 MiB)\n",
      "22/09/07 10:28:17 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:28:17 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 147 (MapPartitionsRDD[393] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:28:17 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:28:17 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 143) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:28:17 INFO Executor: Running task 0.0 in stage 147.0 (TID 143)\n",
      "22/09/07 10:28:17 INFO JDBCRDD: closed connection\n",
      "22/09/07 10:28:17 INFO Executor: Finished task 0.0 in stage 147.0 (TID 143). 1790 bytes result sent to driver\n",
      "22/09/07 10:28:17 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 143) in 32 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:28:17 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:28:17 INFO DAGScheduler: ResultStage 147 (showString at <unknown>:0) finished in 0.038 s\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Job 123 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:28:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 147: Stage finished\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Job 123 finished: showString at <unknown>:0, took 0.040033 s\n",
      "22/09/07 10:28:17 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Got job 124 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Final stage: ResultStage 148 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Submitting ResultStage 148 (MapPartitionsRDD[396] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:28:17 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 12.7 KiB, free 434.3 MiB)\n",
      "22/09/07 10:28:17 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.3 MiB)\n",
      "22/09/07 10:28:17 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:28:17 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 148 (MapPartitionsRDD[396] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:28:17 INFO TaskSchedulerImpl: Adding task set 148.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:28:17 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 144) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:28:17 INFO Executor: Running task 0.0 in stage 148.0 (TID 144)\n",
      "22/09/07 10:28:17 INFO JDBCRDD: closed connection\n",
      "22/09/07 10:28:17 INFO Executor: Finished task 0.0 in stage 148.0 (TID 144). 1790 bytes result sent to driver\n",
      "22/09/07 10:28:17 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 144) in 19 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:28:17 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:28:17 INFO DAGScheduler: ResultStage 148 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.023 s\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Job 124 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:28:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 148: Stage finished\n",
      "22/09/07 10:28:17 INFO DAGScheduler: Job 124 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.026429 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-----+-----+-------------------+-------+-------+\n|id_pk|level|        update_time|   data|new_col|\n+-----+-----+-------------------+-------+-------+\n|    1|ERROR|1970-01-01 10:13:54| Data 1|   null|\n|    2|ERROR|1970-01-01 10:13:54| Data 2|   null|\n|    3|ERROR|1970-01-01 10:13:54| Data 3|   null|\n|    4|ERROR|1970-01-01 10:13:54| Data 4|   null|\n|    5|ERROR|1970-01-01 10:13:54| Data 5|   null|\n|    6|ERROR|1970-01-01 10:13:54| Data 6|   null|\n|    7|ERROR|1970-01-01 10:13:54| Data 7|   null|\n|    8|ERROR|1970-01-01 10:13:54| Data 8|   null|\n|    9|ERROR|1970-01-01 10:13:54| Data 9|   null|\n|   10|ERROR|1970-01-01 10:13:54|Data 10|   null|\n|   11|ERROR|1970-01-01 10:13:54|Data 11|   null|\n|   12|ERROR|1970-01-01 10:13:54|Data 12|   null|\n|   13|ERROR|1970-01-01 10:13:54|Data 13|   null|\n|   14|ERROR|1970-01-01 10:13:54|Data 14|   null|\n|   15|ERROR|1970-01-01 10:13:54|Data 15|   null|\n|   16|ERROR|1970-01-01 10:13:54|Data 16|   null|\n|   17|ERROR|1970-01-01 10:13:54|Data 17|   null|\n|   18|ERROR|1970-01-01 10:13:54|Data 18|   null|\n|   19|ERROR|1970-01-01 10:13:54|Data 19|   null|\n|   20|ERROR|1970-01-01 10:13:54|Data 20|   null|\n+-----+-----+-------------------+-------+-------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>id_pk</th><th>level</th><th>update_time</th><th>data</th><th>new_col</th></tr>\n<tr><td>1</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 1</td><td>null</td></tr>\n<tr><td>2</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 2</td><td>null</td></tr>\n<tr><td>3</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 3</td><td>null</td></tr>\n<tr><td>4</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 4</td><td>null</td></tr>\n<tr><td>5</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 5</td><td>null</td></tr>\n<tr><td>6</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 6</td><td>null</td></tr>\n<tr><td>7</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 7</td><td>null</td></tr>\n<tr><td>8</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 8</td><td>null</td></tr>\n<tr><td>9</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 9</td><td>null</td></tr>\n<tr><td>10</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 10</td><td>null</td></tr>\n<tr><td>11</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 11</td><td>null</td></tr>\n<tr><td>12</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 12</td><td>null</td></tr>\n<tr><td>13</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 13</td><td>null</td></tr>\n<tr><td>14</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 14</td><td>null</td></tr>\n<tr><td>15</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 15</td><td>null</td></tr>\n<tr><td>16</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 16</td><td>null</td></tr>\n<tr><td>17</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 17</td><td>null</td></tr>\n<tr><td>18</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 18</td><td>null</td></tr>\n<tr><td>19</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 19</td><td>null</td></tr>\n<tr><td>20</td><td>ERROR</td><td>1970-01-01 10:13:54</td><td>Data 20</td><td>null</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"new_col\", F.lit(None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:23:01 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:23:01 INFO DAGScheduler: Got job 59 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:23:01 INFO DAGScheduler: Final stage: ResultStage 72 (showString at <unknown>:0)\n",
      "22/09/07 10:23:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:01 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:01 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[231] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:23:01 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:01 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:01 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 192.168.1.81:39211 (size: 6.4 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:01 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[231] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:23:01 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:23:01 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 59) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:01 INFO Executor: Running task 0.0 in stage 72.0 (TID 59)\n",
      "22/09/07 10:23:02 INFO Executor: Finished task 0.0 in stage 72.0 (TID 59). 1847 bytes result sent to driver\n",
      "22/09/07 10:23:02 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 59) in 36 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:23:02 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:02 INFO DAGScheduler: ResultStage 72 (showString at <unknown>:0) finished in 0.045 s\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Job 59 finished: showString at <unknown>:0, took 0.047694 s\n",
      "22/09/07 10:23:02 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Got job 60 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Final stage: ResultStage 73 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[233] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:23:02 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 12.0 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:02 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:02 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 192.168.1.81:39211 (size: 6.4 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:02 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[233] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:23:02 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:23:02 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 60) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:02 INFO Executor: Running task 0.0 in stage 73.0 (TID 60)\n",
      "22/09/07 10:23:02 INFO Executor: Finished task 0.0 in stage 73.0 (TID 60). 1847 bytes result sent to driver\n",
      "22/09/07 10:23:02 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 60) in 46 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:23:02 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:02 INFO DAGScheduler: ResultStage 73 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.053 s\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished\n",
      "22/09/07 10:23:02 INFO DAGScheduler: Job 60 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.055997 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-------+\n|Celsius|\n+-------+\n|   -100|\n|    -99|\n|    -98|\n|    -97|\n|    -96|\n|    -95|\n|    -94|\n|    -93|\n|    -92|\n|    -91|\n|    -90|\n|    -89|\n|    -88|\n|    -87|\n|    -86|\n|    -85|\n|    -84|\n|    -83|\n|    -82|\n|    -81|\n+-------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>Celsius</th></tr>\n<tr><td>-100</td></tr>\n<tr><td>-99</td></tr>\n<tr><td>-98</td></tr>\n<tr><td>-97</td></tr>\n<tr><td>-96</td></tr>\n<tr><td>-95</td></tr>\n<tr><td>-94</td></tr>\n<tr><td>-93</td></tr>\n<tr><td>-92</td></tr>\n<tr><td>-91</td></tr>\n<tr><td>-90</td></tr>\n<tr><td>-89</td></tr>\n<tr><td>-88</td></tr>\n<tr><td>-87</td></tr>\n<tr><td>-86</td></tr>\n<tr><td>-85</td></tr>\n<tr><td>-84</td></tr>\n<tr><td>-83</td></tr>\n<tr><td>-82</td></tr>\n<tr><td>-81</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "temp_celsius = [(item,) for item in range(-100, 100)]\n",
    "schema = StructType([\n",
    "    StructField(\"Celsius\", IntegerType())\n",
    "])\n",
    "newDF = session.createDataFrame(data=temp_celsius, schema=schema)\n",
    "newDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:23:05 INFO CodeGenerator: Code generated in 14.966792 ms\n",
      "22/09/07 10:23:05 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Got job 62 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Final stage: ResultStage 75 (showString at <unknown>:0)\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[238] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 13.2 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:05 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 192.168.1.81:39211 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:05 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[238] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 62) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:05 INFO Executor: Running task 0.0 in stage 75.0 (TID 62)\n",
      "22/09/07 10:23:05 INFO PythonRunner: Times: total = 8, boot = 4, init = 4, finish = 0\n",
      "22/09/07 10:23:05 INFO Executor: Finished task 0.0 in stage 75.0 (TID 62). 1845 bytes result sent to driver\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 62) in 18 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:05 INFO DAGScheduler: ResultStage 75 (showString at <unknown>:0) finished in 0.022 s\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Job 62 finished: showString at <unknown>:0, took 0.023959 s\n",
      "22/09/07 10:23:05 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Got job 63 (showString at <unknown>:0) with 2 output partitions\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Final stage: ResultStage 76 (showString at <unknown>:0)\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[238] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 13.2 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:05 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 192.168.1.81:39211 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:05 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 76 (MapPartitionsRDD[238] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Adding task set 76.0 with 2 tasks resource profile 0\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 63) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4825 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Starting task 1.0 in stage 76.0 (TID 64) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4757 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:05 INFO Executor: Running task 0.0 in stage 76.0 (TID 63)\n",
      "22/09/07 10:23:05 INFO Executor: Running task 1.0 in stage 76.0 (TID 64)\n",
      "22/09/07 10:23:05 INFO Executor: Finished task 0.0 in stage 76.0 (TID 63). 2047 bytes result sent to driver\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 63) in 36 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/07 10:23:05 INFO Executor: Finished task 1.0 in stage 76.0 (TID 64). 2070 bytes result sent to driver\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Finished task 1.0 in stage 76.0 (TID 64) in 45 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:05 INFO DAGScheduler: ResultStage 76 (showString at <unknown>:0) finished in 0.052 s\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Job 63 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Job 63 finished: showString at <unknown>:0, took 0.057008 s\n",
      "22/09/07 10:23:05 INFO CodeGenerator: Code generated in 8.670292 ms\n",
      "22/09/07 10:23:05 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Got job 64 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Final stage: ResultStage 77 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[240] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 13.2 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:05 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 192.168.1.81:39211 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:05 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[240] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 65) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:05 INFO Executor: Running task 0.0 in stage 77.0 (TID 65)\n",
      "22/09/07 10:23:05 INFO PythonRunner: Times: total = 15, boot = 8, init = 6, finish = 1\n",
      "22/09/07 10:23:05 INFO Executor: Finished task 0.0 in stage 77.0 (TID 65). 1845 bytes result sent to driver\n",
      "22/09/07 10:23:05 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 65) in 40 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:05 INFO DAGScheduler: ResultStage 77 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.046 s\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Job 64 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.048050 s\n",
      "22/09/07 10:23:05 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Got job 65 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Final stage: ResultStage 78 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:05 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[240] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:23:05 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 13.2 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:06 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:06 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 192.168.1.81:39211 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:06 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:06 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 78 (MapPartitionsRDD[240] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/07 10:23:06 INFO TaskSchedulerImpl: Adding task set 78.0 with 2 tasks resource profile 0\n",
      "22/09/07 10:23:06 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 66) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4825 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:06 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 192.168.1.81:39211 in memory (size: 6.4 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:06 INFO TaskSetManager: Starting task 1.0 in stage 78.0 (TID 67) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4757 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:06 INFO Executor: Running task 1.0 in stage 78.0 (TID 67)\n",
      "22/09/07 10:23:06 INFO Executor: Running task 0.0 in stage 78.0 (TID 66)\n",
      "22/09/07 10:23:06 INFO Executor: Finished task 1.0 in stage 78.0 (TID 67). 2070 bytes result sent to driver\n",
      "22/09/07 10:23:06 INFO TaskSetManager: Finished task 1.0 in stage 78.0 (TID 67) in 76 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/07 10:23:06 INFO Executor: Finished task 0.0 in stage 78.0 (TID 66). 2047 bytes result sent to driver\n",
      "22/09/07 10:23:06 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 66) in 80 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/07 10:23:06 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:06 INFO DAGScheduler: ResultStage 78 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.164 s\n",
      "22/09/07 10:23:06 INFO DAGScheduler: Job 65 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished\n",
      "22/09/07 10:23:06 INFO DAGScheduler: Job 65 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.170021 s\n",
      "22/09/07 10:23:06 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 192.168.1.81:39211 in memory (size: 6.4 KiB, free: 434.3 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-------+------------------+\n|Celsius|         Farengate|\n+-------+------------------+\n|      0|              32.0|\n|      1|              33.8|\n|      2|              35.6|\n|      3|              37.4|\n|      4|              39.2|\n|      5|              41.0|\n|      6|              42.8|\n|      7|              44.6|\n|      8|              46.4|\n|      9|              48.2|\n|     10|              50.0|\n|     11|              51.8|\n|     12|              53.6|\n|     13|55.400000000000006|\n|     14|              57.2|\n|     15|              59.0|\n|     16|              60.8|\n|     17|              62.6|\n|     18|              64.4|\n|     19|              66.2|\n+-------+------------------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>Celsius</th><th>Farengate</th></tr>\n<tr><td>0</td><td>32.0</td></tr>\n<tr><td>1</td><td>33.8</td></tr>\n<tr><td>2</td><td>35.6</td></tr>\n<tr><td>3</td><td>37.4</td></tr>\n<tr><td>4</td><td>39.2</td></tr>\n<tr><td>5</td><td>41.0</td></tr>\n<tr><td>6</td><td>42.8</td></tr>\n<tr><td>7</td><td>44.6</td></tr>\n<tr><td>8</td><td>46.4</td></tr>\n<tr><td>9</td><td>48.2</td></tr>\n<tr><td>10</td><td>50.0</td></tr>\n<tr><td>11</td><td>51.8</td></tr>\n<tr><td>12</td><td>53.6</td></tr>\n<tr><td>13</td><td>55.400000000000006</td></tr>\n<tr><td>14</td><td>57.2</td></tr>\n<tr><td>15</td><td>59.0</td></tr>\n<tr><td>16</td><td>60.8</td></tr>\n<tr><td>17</td><td>62.6</td></tr>\n<tr><td>18</td><td>64.4</td></tr>\n<tr><td>19</td><td>66.2</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDF.withColumn(\"Farengate\", F.col(\"Celsius\") * (9 / 5) + 32).filter(F.col(\"Celsius\") >= 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "people = [(\"Jon\", 1,), (\"Mark\", 2), (\"Oleg\", 1)]\n",
    "column_people = [\"name\", \"departament\"]\n",
    "departaments = [(1, \"Dep_1\"), (2, \"Dep_2\")]\n",
    "columns_departaments = [\"id\", \"name_dep\"]\n",
    "peopleDF = session.createDataFrame(people, schema=column_people)\n",
    "departDF = session.createDataFrame(departaments, schema=columns_departaments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:29:20 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Got job 131 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Final stage: ResultStage 155 (showString at <unknown>:0)\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Submitting ResultStage 155 (MapPartitionsRDD[418] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:29:20 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 12.4 KiB, free 434.2 MiB)\n",
      "22/09/07 10:29:20 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.2 MiB)\n",
      "22/09/07 10:29:20 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:20 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 155 (MapPartitionsRDD[418] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:29:20 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:29:20 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 153) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4468 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:29:20 INFO Executor: Running task 0.0 in stage 155.0 (TID 153)\n",
      "22/09/07 10:29:20 INFO PythonRunner: Times: total = 5, boot = -695, init = 700, finish = 0\n",
      "22/09/07 10:29:20 INFO Executor: Finished task 0.0 in stage 155.0 (TID 153). 1832 bytes result sent to driver\n",
      "22/09/07 10:29:20 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 153) in 14 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:29:20 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:29:20 INFO DAGScheduler: ResultStage 155 (showString at <unknown>:0) finished in 0.020 s\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Job 131 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:29:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 155: Stage finished\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Job 131 finished: showString at <unknown>:0, took 0.022666 s\n",
      "22/09/07 10:29:20 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Got job 132 (showString at <unknown>:0) with 2 output partitions\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Final stage: ResultStage 156 (showString at <unknown>:0)\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[418] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:29:20 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:29:20 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:29:20 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:20 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:29:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 156 (MapPartitionsRDD[418] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/07 10:29:20 INFO TaskSchedulerImpl: Adding task set 156.0 with 2 tasks resource profile 0\n",
      "22/09/07 10:29:20 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 154) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:29:20 INFO TaskSetManager: Starting task 1.0 in stage 156.0 (TID 155) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:29:20 INFO Executor: Running task 0.0 in stage 156.0 (TID 154)\n",
      "22/09/07 10:29:20 INFO Executor: Running task 1.0 in stage 156.0 (TID 155)\n",
      "22/09/07 10:29:20 INFO PythonRunner: Times: total = 16, boot = -664, init = 680, finish = 0\n",
      "22/09/07 10:29:20 INFO Executor: Finished task 1.0 in stage 156.0 (TID 155). 1833 bytes result sent to driver\n",
      "22/09/07 10:29:20 INFO TaskSetManager: Finished task 1.0 in stage 156.0 (TID 155) in 30 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/07 10:29:21 INFO PythonRunner: Times: total = 56, boot = 28, init = 27, finish = 1\n",
      "22/09/07 10:29:21 INFO Executor: Finished task 0.0 in stage 156.0 (TID 154). 1833 bytes result sent to driver\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 154) in 60 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:29:21 INFO DAGScheduler: ResultStage 156 (showString at <unknown>:0) finished in 0.063 s\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Job 132 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Job 132 finished: showString at <unknown>:0, took 0.065400 s\n",
      "22/09/07 10:29:21 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Got job 133 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Final stage: ResultStage 157 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[420] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:29:21 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:29:21 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 157 (MapPartitionsRDD[420] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_127_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 156) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4468 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:29:21 INFO Executor: Running task 0.0 in stage 157.0 (TID 156)\n",
      "22/09/07 10:29:21 INFO PythonRunner: Times: total = 1, boot = -90, init = 91, finish = 0\n",
      "22/09/07 10:29:21 INFO Executor: Finished task 0.0 in stage 157.0 (TID 156). 1832 bytes result sent to driver\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 156) in 5 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:29:21 INFO DAGScheduler: ResultStage 157 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.046 s\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Job 133 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Job 133 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.046866 s\n",
      "22/09/07 10:29:21 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Got job 134 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Final stage: ResultStage 158 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[420] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:29:21 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:29:21 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 158 (MapPartitionsRDD[420] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Adding task set 158.0 with 2 tasks resource profile 0\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 157) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Starting task 1.0 in stage 158.0 (TID 158) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:29:21 INFO Executor: Running task 1.0 in stage 158.0 (TID 158)\n",
      "22/09/07 10:29:21 INFO Executor: Running task 0.0 in stage 158.0 (TID 157)\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_132_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO PythonRunner: Times: total = 10, boot = -69, init = 79, finish = 0\n",
      "22/09/07 10:29:21 INFO Executor: Finished task 1.0 in stage 158.0 (TID 158). 1833 bytes result sent to driver\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Finished task 1.0 in stage 158.0 (TID 158) in 30 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_124_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_122_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO PythonRunner: Times: total = 54, boot = -12, init = 66, finish = 0\n",
      "22/09/07 10:29:21 INFO Executor: Finished task 0.0 in stage 158.0 (TID 157). 1833 bytes result sent to driver\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_126_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 157) in 74 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:29:21 INFO DAGScheduler: ResultStage 158 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.078 s\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Job 134 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:29:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished\n",
      "22/09/07 10:29:21 INFO BlockManagerInfo: Removed broadcast_130_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:29:21 INFO DAGScheduler: Job 134 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.081335 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+----+-----------+\n|name|departament|\n+----+-----------+\n| Jon|          1|\n|Mark|          2|\n|Oleg|          1|\n+----+-----------+",
      "text/html": "<table border='1'>\n<tr><th>name</th><th>departament</th></tr>\n<tr><td>Jon</td><td>1</td></tr>\n<tr><td>Mark</td><td>2</td></tr>\n<tr><td>Oleg</td><td>1</td></tr>\n</table>\n"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:23:45 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Got job 86 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Final stage: ResultStage 99 (showString at <unknown>:0)\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting ResultStage 99 (MapPartitionsRDD[280] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 12.4 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:45 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:45 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 99 (MapPartitionsRDD[280] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 96) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:45 INFO Executor: Running task 0.0 in stage 99.0 (TID 96)\n",
      "22/09/07 10:23:45 INFO PythonRunner: Times: total = 1, boot = -15892, init = 15893, finish = 0\n",
      "22/09/07 10:23:45 INFO Executor: Finished task 0.0 in stage 99.0 (TID 96). 1789 bytes result sent to driver\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 96) in 16 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:45 INFO DAGScheduler: ResultStage 99 (showString at <unknown>:0) finished in 0.023 s\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 99: Stage finished\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 86 finished: showString at <unknown>:0, took 0.026902 s\n",
      "22/09/07 10:23:45 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Got job 87 (showString at <unknown>:0) with 2 output partitions\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Final stage: ResultStage 100 (showString at <unknown>:0)\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[280] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 12.4 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.2 MiB)\n",
      "22/09/07 10:23:45 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:45 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 100 (MapPartitionsRDD[280] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Adding task set 100.0 with 2 tasks resource profile 0\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 97) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Starting task 1.0 in stage 100.0 (TID 98) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:45 INFO Executor: Running task 1.0 in stage 100.0 (TID 98)\n",
      "22/09/07 10:23:45 INFO Executor: Running task 0.0 in stage 100.0 (TID 97)\n",
      "22/09/07 10:23:45 INFO PythonRunner: Times: total = 6, boot = -15862, init = 15868, finish = 0\n",
      "22/09/07 10:23:45 INFO Executor: Finished task 1.0 in stage 100.0 (TID 98). 1830 bytes result sent to driver\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Finished task 1.0 in stage 100.0 (TID 98) in 20 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/07 10:23:45 INFO PythonRunner: Times: total = 75, boot = -28, init = 103, finish = 0\n",
      "22/09/07 10:23:45 INFO Executor: Finished task 0.0 in stage 100.0 (TID 97). 1830 bytes result sent to driver\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 97) in 86 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:45 INFO DAGScheduler: ResultStage 100 (showString at <unknown>:0) finished in 0.103 s\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 87 finished: showString at <unknown>:0, took 0.111081 s\n",
      "22/09/07 10:23:45 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Got job 88 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Final stage: ResultStage 101 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting ResultStage 101 (MapPartitionsRDD[282] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:45 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:45 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 101 (MapPartitionsRDD[282] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 99) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:45 INFO Executor: Running task 0.0 in stage 101.0 (TID 99)\n",
      "22/09/07 10:23:45 INFO PythonRunner: Times: total = 3, boot = -100, init = 103, finish = 0\n",
      "22/09/07 10:23:45 INFO Executor: Finished task 0.0 in stage 101.0 (TID 99). 1789 bytes result sent to driver\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 99) in 22 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:45 INFO DAGScheduler: ResultStage 101 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.026 s\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 101: Stage finished\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 88 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.028873 s\n",
      "22/09/07 10:23:45 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Got job 89 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Final stage: ResultStage 102 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting ResultStage 102 (MapPartitionsRDD[282] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:45 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/07 10:23:45 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on 192.168.1.81:39211 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:23:45 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 102 (MapPartitionsRDD[282] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Adding task set 102.0 with 2 tasks resource profile 0\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 100) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Starting task 1.0 in stage 102.0 (TID 101) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:23:45 INFO Executor: Running task 1.0 in stage 102.0 (TID 101)\n",
      "22/09/07 10:23:45 INFO Executor: Running task 0.0 in stage 102.0 (TID 100)\n",
      "22/09/07 10:23:45 INFO PythonRunner: Times: total = 2, boot = -54, init = 56, finish = 0\n",
      "22/09/07 10:23:45 INFO Executor: Finished task 1.0 in stage 102.0 (TID 101). 1830 bytes result sent to driver\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Finished task 1.0 in stage 102.0 (TID 101) in 8 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/07 10:23:45 INFO PythonRunner: Times: total = 48, boot = 3, init = 45, finish = 0\n",
      "22/09/07 10:23:45 INFO Executor: Finished task 0.0 in stage 102.0 (TID 100). 1830 bytes result sent to driver\n",
      "22/09/07 10:23:45 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 100) in 54 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:23:45 INFO DAGScheduler: ResultStage 102 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.060 s\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 89 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:23:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 102: Stage finished\n",
      "22/09/07 10:23:45 INFO DAGScheduler: Job 89 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.061421 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+---+--------+\n| id|name_dep|\n+---+--------+\n|  1|   Dep_1|\n|  2|   Dep_2|\n+---+--------+",
      "text/html": "<table border='1'>\n<tr><th>id</th><th>name_dep</th></tr>\n<tr><td>1</td><td>Dep_1</td></tr>\n<tr><td>2</td><td>Dep_2</td></tr>\n</table>\n"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 8.966422 ms\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Registering RDD 287 (showString at <unknown>:0) as input to shuffle 13\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Got map stage job 91 (showString at <unknown>:0) with 3 output partitions\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Final stage: ShuffleMapStage 104 (showString at <unknown>:0)\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Submitting ShuffleMapStage 104 (MapPartitionsRDD[287] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:24:06 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 15.2 KiB, free 434.1 MiB)\n",
      "22/09/07 10:24:06 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 192.168.1.81:39211 (size: 8.0 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 104 (MapPartitionsRDD[287] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/07 10:24:06 INFO TaskSchedulerImpl: Adding task set 104.0 with 3 tasks resource profile 0\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 103) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Starting task 1.0 in stage 104.0 (TID 104) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4458 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Starting task 2.0 in stage 104.0 (TID 105) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4458 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:06 INFO Executor: Running task 1.0 in stage 104.0 (TID 104)\n",
      "22/09/07 10:24:06 INFO Executor: Running task 0.0 in stage 104.0 (TID 103)\n",
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 19.146526 ms\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Registering RDD 289 (showString at <unknown>:0) as input to shuffle 14\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Got map stage job 92 (showString at <unknown>:0) with 3 output partitions\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Final stage: ShuffleMapStage 105 (showString at <unknown>:0)\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Submitting ShuffleMapStage 105 (MapPartitionsRDD[289] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:24:06 INFO Executor: Running task 2.0 in stage 104.0 (TID 105)\n",
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 20.716911 ms\n",
      "22/09/07 10:24:06 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 15.2 KiB, free 434.1 MiB)\n",
      "22/09/07 10:24:06 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.1 MiB)\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 192.168.1.81:39211 (size: 8.0 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:24:06 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 105 (MapPartitionsRDD[289] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/07 10:24:06 INFO TaskSchedulerImpl: Adding task set 105.0 with 3 tasks resource profile 0\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_78_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO PythonRunner: Times: total = 8, boot = -20782, init = 20790, finish = 0\n",
      "22/09/07 10:24:06 INFO PythonRunner: Times: total = 21, boot = -20826, init = 20846, finish = 1\n",
      "22/09/07 10:24:06 INFO PythonRunner: Times: total = 23, boot = 3, init = 20, finish = 0\n",
      "22/09/07 10:24:06 INFO Executor: Finished task 1.0 in stage 104.0 (TID 104). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 106) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4422 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:06 INFO Executor: Running task 0.0 in stage 105.0 (TID 106)\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Finished task 1.0 in stage 104.0 (TID 104) in 82 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/07 10:24:06 INFO Executor: Finished task 0.0 in stage 104.0 (TID 103). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Starting task 1.0 in stage 105.0 (TID 107) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4459 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 103) in 102 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/07 10:24:06 INFO Executor: Running task 1.0 in stage 105.0 (TID 107)\n",
      "22/09/07 10:24:06 INFO Executor: Finished task 2.0 in stage 104.0 (TID 105). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Starting task 2.0 in stage 105.0 (TID 108) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4459 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Finished task 2.0 in stage 104.0 (TID 105) in 103 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/07 10:24:06 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:24:06 INFO DAGScheduler: ShuffleMapStage 104 (showString at <unknown>:0) finished in 0.136 s\n",
      "22/09/07 10:24:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:24:06 INFO DAGScheduler: running: Set(ShuffleMapStage 105)\n",
      "22/09/07 10:24:06 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:24:06 INFO Executor: Running task 2.0 in stage 105.0 (TID 108)\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_90_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 9.037148 ms\n",
      "22/09/07 10:24:06 INFO PythonRunner: Times: total = 59, boot = -77, init = 136, finish = 0\n",
      "22/09/07 10:24:06 INFO PythonRunner: Times: total = 92, boot = -38, init = 130, finish = 0\n",
      "22/09/07 10:24:06 INFO Executor: Finished task 0.0 in stage 105.0 (TID 106). 2260 bytes result sent to driver\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 106) in 175 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/07 10:24:06 INFO PythonRunner: Times: total = 92, boot = -22, init = 114, finish = 0\n",
      "22/09/07 10:24:06 INFO Executor: Finished task 1.0 in stage 105.0 (TID 107). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Finished task 1.0 in stage 105.0 (TID 107) in 160 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/07 10:24:06 INFO Executor: Finished task 2.0 in stage 105.0 (TID 108). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:06 INFO TaskSetManager: Finished task 2.0 in stage 105.0 (TID 108) in 160 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/07 10:24:06 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:24:06 INFO DAGScheduler: ShuffleMapStage 105 (showString at <unknown>:0) finished in 0.240 s\n",
      "22/09/07 10:24:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:24:06 INFO DAGScheduler: running: Set()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:24:06 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO ShufflePartitionsUtil: For shuffle(13, 14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_85_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 23.562633 ms\n",
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 11.739497 ms\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_88_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:06 INFO CodeGenerator: Code generated in 15.622549 ms\n",
      "22/09/07 10:24:06 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Removed broadcast_86_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Got job 93 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Final stage: ResultStage 108 (showString at <unknown>:0)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 107, ShuffleMapStage 106)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting ResultStage 108 (MapPartitionsRDD[296] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 41.1 KiB, free 434.2 MiB)\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on 192.168.1.81:39211 (size: 18.4 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 108 (MapPartitionsRDD[296] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 109) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO Executor: Running task 0.0 in stage 108.0 (TID 109)\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Getting 3 (216.0 B) non-empty blocks including 3 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Removed broadcast_87_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO CodeGenerator: Code generated in 30.935346 ms\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO CodeGenerator: Code generated in 5.970552 ms\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Removed broadcast_89_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO CodeGenerator: Code generated in 7.470112 ms\n",
      "22/09/07 10:24:07 INFO CodeGenerator: Code generated in 4.593166 ms\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 192.168.1.81:39211 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 0.0 in stage 108.0 (TID 109). 4648 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 109) in 145 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:24:07 INFO DAGScheduler: ResultStage 108 (showString at <unknown>:0) finished in 0.153 s\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Job 93 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 108: Stage finished\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Job 93 finished: showString at <unknown>:0, took 0.191244 s\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Registering RDD 298 (getRowsToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 15\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Got map stage job 94 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Final stage: ShuffleMapStage 109 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting ShuffleMapStage 109 (MapPartitionsRDD[298] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 15.2 KiB, free 434.2 MiB)\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.2 MiB)\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on 192.168.1.81:39211 (size: 8.0 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 109 (MapPartitionsRDD[298] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Adding task set 109.0 with 3 tasks resource profile 0\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 110) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 1.0 in stage 109.0 (TID 111) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4458 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 2.0 in stage 109.0 (TID 112) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4458 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO Executor: Running task 0.0 in stage 109.0 (TID 110)\n",
      "22/09/07 10:24:07 INFO Executor: Running task 1.0 in stage 109.0 (TID 111)\n",
      "22/09/07 10:24:07 INFO Executor: Running task 2.0 in stage 109.0 (TID 112)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Registering RDD 300 (getRowsToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 16\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Got map stage job 95 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Final stage: ShuffleMapStage 110 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[300] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 15.2 KiB, free 434.2 MiB)\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.2 MiB)\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on 192.168.1.81:39211 (size: 8.0 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[300] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Adding task set 110.0 with 3 tasks resource profile 0\n",
      "22/09/07 10:24:07 INFO PythonRunner: Times: total = 5, boot = -506, init = 511, finish = 0\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 2.0 in stage 109.0 (TID 112). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 113) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4422 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 2.0 in stage 109.0 (TID 112) in 50 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/07 10:24:07 INFO Executor: Running task 0.0 in stage 110.0 (TID 113)\n",
      "22/09/07 10:24:07 INFO PythonRunner: Times: total = 51, boot = -475, init = 526, finish = 0\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 0.0 in stage 109.0 (TID 110). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 114) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4459 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 110) in 63 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/07 10:24:07 INFO Executor: Running task 1.0 in stage 110.0 (TID 114)\n",
      "22/09/07 10:24:07 INFO PythonRunner: Times: total = 40, boot = -506, init = 546, finish = 0\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 1.0 in stage 109.0 (TID 111). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 2.0 in stage 110.0 (TID 115) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4459 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 1.0 in stage 109.0 (TID 111) in 81 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:24:07 INFO Executor: Running task 2.0 in stage 110.0 (TID 115)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: ShuffleMapStage 109 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.090 s\n",
      "22/09/07 10:24:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:24:07 INFO DAGScheduler: running: Set(ShuffleMapStage 110)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:24:07 INFO PythonRunner: Times: total = 7, boot = -15, init = 22, finish = 0\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 2.0 in stage 110.0 (TID 115). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 2.0 in stage 110.0 (TID 115) in 30 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/07 10:24:07 INFO PythonRunner: Times: total = 18, boot = -2, init = 20, finish = 0\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 1.0 in stage 110.0 (TID 114). 2389 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 114) in 54 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/07 10:24:07 INFO PythonRunner: Times: total = 69, boot = -6, init = 75, finish = 0\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 0.0 in stage 110.0 (TID 113). 2260 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 113) in 79 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:24:07 INFO DAGScheduler: ShuffleMapStage 110 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.102 s\n",
      "22/09/07 10:24:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/07 10:24:07 INFO DAGScheduler: running: Set()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: waiting: Set()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: failed: Set()\n",
      "22/09/07 10:24:07 INFO ShufflePartitionsUtil: For shuffle(15, 16), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/07 10:24:07 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Got job 96 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Final stage: ResultStage 113 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 111, ShuffleMapStage 112)\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting ResultStage 113 (MapPartitionsRDD[307] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 41.1 KiB, free 434.2 MiB)\n",
      "22/09/07 10:24:07 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 434.1 MiB)\n",
      "22/09/07 10:24:07 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on 192.168.1.81:39211 (size: 18.4 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:24:07 INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 113 (MapPartitionsRDD[307] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 116) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:24:07 INFO Executor: Running task 0.0 in stage 113.0 (TID 116)\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Getting 3 (216.0 B) non-empty blocks including 3 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/07 10:24:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/07 10:24:07 INFO Executor: Finished task 0.0 in stage 113.0 (TID 116). 4648 bytes result sent to driver\n",
      "22/09/07 10:24:07 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 116) in 14 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:24:07 INFO DAGScheduler: ResultStage 113 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.021 s\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:24:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 113: Stage finished\n",
      "22/09/07 10:24:07 INFO DAGScheduler: Job 96 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.025265 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+----+--------+\n|name|name_dep|\n+----+--------+\n| Jon|   Dep_1|\n|Oleg|   Dep_1|\n|Mark|   Dep_2|\n+----+--------+",
      "text/html": "<table border='1'>\n<tr><th>name</th><th>name_dep</th></tr>\n<tr><td>Jon</td><td>Dep_1</td></tr>\n<tr><td>Oleg</td><td>Dep_1</td></tr>\n<tr><td>Mark</td><td>Dep_2</td></tr>\n</table>\n"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.join(departDF, peopleDF[\"departament\"] == departDF[\"id\"]).select(F.col(\"name\"), F.col(\"name_dep\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:31:15 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:31:15 INFO DAGScheduler: Got job 149 (save at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "22/09/07 10:31:15 INFO DAGScheduler: Final stage: ResultStage 173 (save at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:31:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:31:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:31:15 INFO DAGScheduler: Submitting ResultStage 173 (MapPartitionsRDD[452] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:31:15 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 26.4 KiB, free 434.2 MiB)\n",
      "22/09/07 10:31:15 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 434.2 MiB)\n",
      "22/09/07 10:31:15 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on 192.168.1.81:39211 (size: 12.9 KiB, free: 434.3 MiB)\n",
      "22/09/07 10:31:15 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:31:15 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 173 (MapPartitionsRDD[452] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/07 10:31:15 INFO TaskSchedulerImpl: Adding task set 173.0 with 3 tasks resource profile 0\n",
      "22/09/07 10:31:15 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 181) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4468 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:31:15 INFO TaskSetManager: Starting task 1.0 in stage 173.0 (TID 182) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:31:15 INFO TaskSetManager: Starting task 2.0 in stage 173.0 (TID 183) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:31:15 INFO Executor: Running task 0.0 in stage 173.0 (TID 181)\n",
      "22/09/07 10:31:15 INFO Executor: Running task 1.0 in stage 173.0 (TID 182)\n",
      "22/09/07 10:31:15 INFO Executor: Running task 2.0 in stage 173.0 (TID 183)\n",
      "22/09/07 10:31:16 INFO PythonRunner: Times: total = 2, boot = -33175, init = 33177, finish = 0\n",
      "22/09/07 10:31:16 INFO Executor: Finished task 0.0 in stage 173.0 (TID 181). 1660 bytes result sent to driver\n",
      "22/09/07 10:31:16 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 181) in 30 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/07 10:31:16 INFO PythonRunner: Times: total = 15, boot = -33128, init = 33143, finish = 0\n",
      "22/09/07 10:31:16 INFO PythonRunner: Times: total = 43, boot = -33192, init = 33234, finish = 1\n",
      "22/09/07 10:31:16 INFO Executor: Finished task 1.0 in stage 173.0 (TID 182). 1660 bytes result sent to driver\n",
      "22/09/07 10:31:16 INFO TaskSetManager: Finished task 1.0 in stage 173.0 (TID 182) in 64 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/07 10:31:16 INFO Executor: Finished task 2.0 in stage 173.0 (TID 183). 1660 bytes result sent to driver\n",
      "22/09/07 10:31:16 INFO TaskSetManager: Finished task 2.0 in stage 173.0 (TID 183) in 66 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/07 10:31:16 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:31:16 INFO DAGScheduler: ResultStage 173 (save at NativeMethodAccessorImpl.java:0) finished in 0.072 s\n",
      "22/09/07 10:31:16 INFO DAGScheduler: Job 149 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:31:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 173: Stage finished\n",
      "22/09/07 10:31:16 INFO DAGScheduler: Job 149 finished: save at NativeMethodAccessorImpl.java:0, took 0.073116 s\n"
     ]
    }
   ],
   "source": [
    "(peopleDF.write\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\n",
    " .option(\"dbtable\", \"out_table\")\n",
    " .option(\"user\", \"user\")\n",
    " .option(\"password\", \"password\")\n",
    " .option(\"driver\", \"org.postgresql.Driver\")\n",
    " # .mode(\"overwrite\")\n",
    " .mode(\"append\")\n",
    " .save())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 10:39:52 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "22/09/07 10:39:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "22/09/07 10:39:52 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/09/07 10:39:52 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#3207, None)) > 0)\n",
      "22/09/07 10:39:52 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_201 stored as values in memory (estimated size 200.1 KiB, free 433.7 MiB)\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_201_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.6 MiB)\n",
      "22/09/07 10:39:52 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on 192.168.1.81:39211 (size: 34.4 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:39:52 INFO SparkContext: Created broadcast 201 from csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:39:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/09/07 10:39:52 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Got job 188 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Final stage: ResultStage 212 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Submitting ResultStage 212 (MapPartitionsRDD[554] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_202 stored as values in memory (estimated size 11.8 KiB, free 433.6 MiB)\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_202_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.6 MiB)\n",
      "22/09/07 10:39:52 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on 192.168.1.81:39211 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:39:52 INFO SparkContext: Created broadcast 202 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 212 (MapPartitionsRDD[554] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:39:52 INFO TaskSchedulerImpl: Adding task set 212.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:39:52 INFO TaskSetManager: Starting task 0.0 in stage 212.0 (TID 231) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:39:52 INFO Executor: Running task 0.0 in stage 212.0 (TID 231)\n",
      "22/09/07 10:39:52 INFO FileScanRDD: Reading File path: file:///home/dmitryrusack/Work/about_spark/local_master/dataFrame_ex/test_file.csv, range: 0-127679, partition values: [empty row]\n",
      "22/09/07 10:39:52 INFO Executor: Finished task 0.0 in stage 212.0 (TID 231). 1558 bytes result sent to driver\n",
      "22/09/07 10:39:52 INFO TaskSetManager: Finished task 0.0 in stage 212.0 (TID 231) in 9 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:39:52 INFO TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:39:52 INFO DAGScheduler: ResultStage 212 (csv at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Job 188 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:39:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 212: Stage finished\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Job 188 finished: csv at NativeMethodAccessorImpl.java:0, took 0.014624 s\n",
      "22/09/07 10:39:52 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/09/07 10:39:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/09/07 10:39:52 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_203 stored as values in memory (estimated size 200.1 KiB, free 433.4 MiB)\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_203_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.4 MiB)\n",
      "22/09/07 10:39:52 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on 192.168.1.81:39211 (size: 34.4 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:39:52 INFO SparkContext: Created broadcast 203 from csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:39:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/09/07 10:39:52 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Got job 189 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Final stage: ResultStage 213 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Submitting ResultStage 213 (MapPartitionsRDD[560] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_204 stored as values in memory (estimated size 25.0 KiB, free 433.4 MiB)\n",
      "22/09/07 10:39:52 INFO MemoryStore: Block broadcast_204_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 433.4 MiB)\n",
      "22/09/07 10:39:52 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on 192.168.1.81:39211 (size: 11.8 KiB, free: 434.2 MiB)\n",
      "22/09/07 10:39:52 INFO SparkContext: Created broadcast 204 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 213 (MapPartitionsRDD[560] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/07 10:39:52 INFO TaskSchedulerImpl: Adding task set 213.0 with 1 tasks resource profile 0\n",
      "22/09/07 10:39:52 INFO TaskSetManager: Starting task 0.0 in stage 213.0 (TID 232) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\n",
      "22/09/07 10:39:52 INFO Executor: Running task 0.0 in stage 213.0 (TID 232)\n",
      "22/09/07 10:39:52 INFO FileScanRDD: Reading File path: file:///home/dmitryrusack/Work/about_spark/local_master/dataFrame_ex/test_file.csv, range: 0-127679, partition values: [empty row]\n",
      "22/09/07 10:39:52 INFO Executor: Finished task 0.0 in stage 213.0 (TID 232). 1571 bytes result sent to driver\n",
      "22/09/07 10:39:52 INFO TaskSetManager: Finished task 0.0 in stage 213.0 (TID 232) in 34 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/07 10:39:52 INFO TaskSchedulerImpl: Removed TaskSet 213.0, whose tasks have all completed, from pool \n",
      "22/09/07 10:39:52 INFO DAGScheduler: ResultStage 213 (csv at NativeMethodAccessorImpl.java:0) finished in 0.039 s\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Job 189 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/07 10:39:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 213: Stage finished\n",
      "22/09/07 10:39:52 INFO DAGScheduler: Job 189 finished: csv at NativeMethodAccessorImpl.java:0, took 0.041841 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType()),\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"message\", StringType())\n",
    "])\n",
    "# df_from_local_file = session.read.csv(\"./test_file.csv\", schema=schema)\n",
    "df_from_local_file = session.read.csv(\"./test_file.csv\", inferSchema=True).repartition(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: timestamp (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_local_file.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/06 18:10:09 INFO SparkUI: Stopped Spark web UI at http://192.168.1.81:4040\n",
      "22/09/06 18:10:09 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/09/06 18:10:09 INFO MemoryStore: MemoryStore cleared\n",
      "22/09/06 18:10:09 INFO BlockManager: BlockManager stopped\n",
      "22/09/06 18:10:09 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/09/06 18:10:09 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/09/06 18:10:09 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "session.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}