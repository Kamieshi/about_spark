{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars ./app_data/postgresql-42.5.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:05:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "session = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars\", \"./postgresql-42.5.0.jar\")\n",
    "    .appName(\"Driver_\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "session.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df = (session.\n",
    "      read.\n",
    "      format(\"jdbc\")\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\n",
    "      .option(\"user\", \"user\")\n",
    "      .option(\"password\", \"password\").option(\"driver\", \"org.postgresql.Driver\")\n",
    "      .option(\"dbtable\", \"test\")\n",
    "      # .option(\"query\", \"SELECT * FROM test WHERE level='ERROR'\")\n",
    "\n",
    "      .load()\n",
    "      )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_pk: integer (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- update_time: timestamp (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 10:53:06 INFO SparkContext: Starting job: first at /tmp/ipykernel_37462/2111424487.py:1\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Got job 7 (first at /tmp/ipykernel_37462/2111424487.py:1) with 1 output partitions\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Final stage: ResultStage 7 (first at /tmp/ipykernel_37462/2111424487.py:1)\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[23] at first at /tmp/ipykernel_37462/2111424487.py:1), which has no missing parents\n",
      "22/09/08 10:53:06 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.2 KiB, free 434.3 MiB)\n",
      "22/09/08 10:53:06 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.3 MiB)\n",
      "22/09/08 10:53:06 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.81:35791 (size: 5.8 KiB, free: 434.4 MiB)\n",
      "22/09/08 10:53:06 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[23] at first at /tmp/ipykernel_37462/2111424487.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 10:53:06 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "22/09/08 10:53:06 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/08 10:53:06 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "22/09/08 10:53:06 INFO JDBCRDD: closed connection\n",
      "22/09/08 10:53:06 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1453 bytes result sent to driver\n",
      "22/09/08 10:53:06 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 35 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 10:53:06 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "22/09/08 10:53:06 INFO DAGScheduler: ResultStage 7 (first at /tmp/ipykernel_37462/2111424487.py:1) finished in 0.045 s\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 10:53:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "22/09/08 10:53:06 INFO DAGScheduler: Job 7 finished: first at /tmp/ipykernel_37462/2111424487.py:1, took 0.048377 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Row(id_pk=1, level='ERROR', update_time=datetime.datetime(1970, 1, 1, 10, 50, 40), data='Data 1')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "n_df = df.repartition(3,\"level\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/09/08 11:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Registering RDD 339 (csv at NativeMethodAccessorImpl.java:0) as input to shuffle 28\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Got map stage job 79 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Final stage: ShuffleMapStage 95 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Submitting ShuffleMapStage 95 (MapPartitionsRDD[339] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:07:34 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 14.2 KiB, free 434.3 MiB)\n",
      "22/09/08 11:07:34 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.3 MiB)\n",
      "22/09/08 11:07:34 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on 192.168.1.81:35791 (size: 7.4 KiB, free: 434.4 MiB)\n",
      "22/09/08 11:07:34 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 95 (MapPartitionsRDD[339] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:07:34 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 101) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:07:34 INFO Executor: Running task 0.0 in stage 95.0 (TID 101)\n",
      "22/09/08 11:07:34 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:07:34 INFO Executor: Finished task 0.0 in stage 95.0 (TID 101). 1742 bytes result sent to driver\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 101) in 31 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:07:34 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:07:34 INFO DAGScheduler: ShuffleMapStage 95 (csv at NativeMethodAccessorImpl.java:0) finished in 0.036 s\n",
      "22/09/08 11:07:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/08 11:07:34 INFO DAGScheduler: running: Set()\n",
      "22/09/08 11:07:34 INFO DAGScheduler: waiting: Set()\n",
      "22/09/08 11:07:34 INFO DAGScheduler: failed: Set()\n",
      "22/09/08 11:07:34 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Got job 80 (csv at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Final stage: ResultStage 97 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 96)\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Submitting ResultStage 97 (ShuffledRowRDD[340] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:07:34 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 209.1 KiB, free 434.1 MiB)\n",
      "22/09/08 11:07:34 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 75.6 KiB, free 434.0 MiB)\n",
      "22/09/08 11:07:34 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on 192.168.1.81:35791 (size: 75.6 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:07:34 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:07:34 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 97 (ShuffledRowRDD[340] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/08 11:07:34 INFO TaskSchedulerImpl: Adding task set 97.0 with 3 tasks resource profile 0\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 102) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Starting task 1.0 in stage 97.0 (TID 103) (192.168.1.81, executor driver, partition 1, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Starting task 2.0 in stage 97.0 (TID 104) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:07:34 INFO Executor: Running task 0.0 in stage 97.0 (TID 102)\n",
      "22/09/08 11:07:34 INFO Executor: Running task 1.0 in stage 97.0 (TID 103)\n",
      "22/09/08 11:07:34 INFO Executor: Running task 2.0 in stage 97.0 (TID 104)\n",
      "22/09/08 11:07:34 INFO ShuffleBlockFetcherIterator: Getting 1 (10.1 KiB) non-empty blocks including 1 (10.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:07:34 INFO ShuffleBlockFetcherIterator: Getting 1 (17.9 KiB) non-empty blocks including 1 (17.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:07:34 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/09/08 11:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:07:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/09/08 11:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/09/08 11:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/09/08 11:07:34 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202209081107342581017908183840763_0097_m_000002_104\n",
      "22/09/08 11:07:34 INFO Executor: Finished task 2.0 in stage 97.0 (TID 104). 3362 bytes result sent to driver\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Finished task 2.0 in stage 97.0 (TID 104) in 42 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/09/08 11:07:34 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/09/08 11:07:34 INFO FileOutputCommitter: Saved output of task 'attempt_202209081107348821341657837071149_0097_m_000001_103' to file:/home/dmitryrusack/Work/about_spark/practic_3/data_after_reportition/_temporary/0/task_202209081107348821341657837071149_0097_m_000001\n",
      "22/09/08 11:07:34 INFO SparkHadoopMapRedUtil: attempt_202209081107348821341657837071149_0097_m_000001_103: Committed. Elapsed time: 0 ms.\n",
      "22/09/08 11:07:34 INFO Executor: Finished task 1.0 in stage 97.0 (TID 103). 3491 bytes result sent to driver\n",
      "22/09/08 11:07:34 INFO TaskSetManager: Finished task 1.0 in stage 97.0 (TID 103) in 103 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/08 11:07:35 INFO FileOutputCommitter: Saved output of task 'attempt_202209081107346031533883468652841_0097_m_000000_102' to file:/home/dmitryrusack/Work/about_spark/practic_3/data_after_reportition/_temporary/0/task_202209081107346031533883468652841_0097_m_000000\n",
      "22/09/08 11:07:35 INFO SparkHadoopMapRedUtil: attempt_202209081107346031533883468652841_0097_m_000000_102: Committed. Elapsed time: 0 ms.\n",
      "22/09/08 11:07:35 INFO Executor: Finished task 0.0 in stage 97.0 (TID 102). 3448 bytes result sent to driver\n",
      "22/09/08 11:07:35 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 102) in 110 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/08 11:07:35 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:07:35 INFO DAGScheduler: ResultStage 97 (csv at NativeMethodAccessorImpl.java:0) finished in 0.128 s\n",
      "22/09/08 11:07:35 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:07:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished\n",
      "22/09/08 11:07:35 INFO DAGScheduler: Job 80 finished: csv at NativeMethodAccessorImpl.java:0, took 0.131451 s\n",
      "22/09/08 11:07:35 INFO FileFormatWriter: Start to commit write Job 6a0bfba8-08e4-42d5-a19a-5129cb58bf3b.\n",
      "22/09/08 11:07:35 INFO FileFormatWriter: Write Job 6a0bfba8-08e4-42d5-a19a-5129cb58bf3b committed. Elapsed time: 17 ms.\n",
      "22/09/08 11:07:35 INFO FileFormatWriter: Finished processing stats for write job 6a0bfba8-08e4-42d5-a19a-5129cb58bf3b.\n"
     ]
    }
   ],
   "source": [
    "n_df.write.csv(\"./data_after_reportition\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 10:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/09/08 10:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/09/08 10:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/09/08 10:53:23 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Got job 9 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Final stage: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[28] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 10:53:23 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 213.1 KiB, free 434.0 MiB)\n",
      "22/09/08 10:53:23 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 77.3 KiB, free 434.0 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.81:35791 (size: 77.3 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[28] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 10:53:23 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "22/09/08 10:53:23 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/08 10:53:23 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "22/09/08 10:53:23 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "22/09/08 10:53:23 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "22/09/08 10:53:23 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.1.81:35791 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.81:35791 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.81:35791 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.81:35791 in memory (size: 5.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 10:53:23 INFO JDBCRDD: closed connection\n",
      "22/09/08 10:53:23 INFO FileOutputCommitter: Saved output of task 'attempt_202209081053239055332023065718075_0009_m_000000_9' to file:/home/dmitryrusack/Work/about_spark/practic_3/data_from_db/_temporary/0/task_202209081053239055332023065718075_0009_m_000000\n",
      "22/09/08 10:53:23 INFO SparkHadoopMapRedUtil: attempt_202209081053239055332023065718075_0009_m_000000_9: Committed. Elapsed time: 1 ms.\n",
      "22/09/08 10:53:23 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 2566 bytes result sent to driver\n",
      "22/09/08 10:53:23 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 380 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 10:53:23 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "22/09/08 10:53:23 INFO DAGScheduler: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0) finished in 0.405 s\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 10:53:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "22/09/08 10:53:23 INFO DAGScheduler: Job 9 finished: csv at NativeMethodAccessorImpl.java:0, took 0.412966 s\n",
      "22/09/08 10:53:23 INFO FileFormatWriter: Start to commit write Job 2048dce5-a9ca-4b08-9f46-0931440e7424.\n",
      "22/09/08 10:53:23 INFO FileFormatWriter: Write Job 2048dce5-a9ca-4b08-9f46-0931440e7424 committed. Elapsed time: 15 ms.\n",
      "22/09/08 10:53:23 INFO FileFormatWriter: Finished processing stats for write job 2048dce5-a9ca-4b08-9f46-0931440e7424.\n"
     ]
    }
   ],
   "source": [
    "df.write.csv(\"./data_from_db\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 49.26831 ms\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Registering RDD 352 (showString at <unknown>:0) as input to shuffle 30\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Got map stage job 84 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Final stage: ShuffleMapStage 102 (showString at <unknown>:0)\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Submitting ShuffleMapStage 102 (MapPartitionsRDD[352] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:08:36 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 33.0 KiB, free 433.9 MiB)\n",
      "22/09/08 11:08:36 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 433.9 MiB)\n",
      "22/09/08 11:08:36 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on 192.168.1.81:35791 (size: 15.5 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:08:36 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 102 (MapPartitionsRDD[352] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:08:36 INFO TaskSchedulerImpl: Adding task set 102.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:08:36 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 108) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:08:36 INFO Executor: Running task 0.0 in stage 102.0 (TID 108)\n",
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 17.314768 ms\n",
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 6.670045 ms\n",
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 6.938017 ms\n",
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 6.722222 ms\n",
      "22/09/08 11:08:36 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:08:36 INFO Executor: Finished task 0.0 in stage 102.0 (TID 108). 2453 bytes result sent to driver\n",
      "22/09/08 11:08:36 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 108) in 90 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:08:36 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:08:36 INFO DAGScheduler: ShuffleMapStage 102 (showString at <unknown>:0) finished in 0.097 s\n",
      "22/09/08 11:08:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/08 11:08:36 INFO DAGScheduler: running: Set()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: waiting: Set()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: failed: Set()\n",
      "22/09/08 11:08:36 INFO ShufflePartitionsUtil: For shuffle(30), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/08 11:08:36 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 17.161948 ms\n",
      "22/09/08 11:08:36 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Got job 85 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Final stage: ResultStage 104 (showString at <unknown>:0)\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 103)\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Submitting ResultStage 104 (MapPartitionsRDD[355] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:08:36 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 34.2 KiB, free 433.8 MiB)\n",
      "22/09/08 11:08:36 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 433.8 MiB)\n",
      "22/09/08 11:08:36 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on 192.168.1.81:35791 (size: 16.3 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:08:36 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 104 (MapPartitionsRDD[355] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:08:36 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:08:36 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 109) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:08:36 INFO Executor: Running task 0.0 in stage 104.0 (TID 109)\n",
      "22/09/08 11:08:36 INFO BlockManagerInfo: Removed broadcast_79_piece0 on 192.168.1.81:35791 in memory (size: 7.4 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:08:36 INFO ShuffleBlockFetcherIterator: Getting 1 (216.0 B) non-empty blocks including 1 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:08:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:08:36 INFO BlockManagerInfo: Removed broadcast_83_piece0 on 192.168.1.81:35791 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:08:36 INFO Executor: Finished task 0.0 in stage 104.0 (TID 109). 3646 bytes result sent to driver\n",
      "22/09/08 11:08:36 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 109) in 23 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:08:36 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:08:36 INFO DAGScheduler: ResultStage 104 (showString at <unknown>:0) finished in 0.046 s\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Job 85 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:08:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 104: Stage finished\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Job 85 finished: showString at <unknown>:0, took 0.049230 s\n",
      "22/09/08 11:08:36 INFO CodeGenerator: Code generated in 14.476868 ms\n",
      "22/09/08 11:08:36 INFO BlockManagerInfo: Removed broadcast_81_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:08:36 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 192.168.1.81:35791 in memory (size: 75.6 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Registering RDD 358 (getRowsToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 31\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Got map stage job 86 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Final stage: ShuffleMapStage 105 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:08:36 INFO DAGScheduler: Submitting ShuffleMapStage 105 (MapPartitionsRDD[358] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:08:37 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 33.1 KiB, free 434.1 MiB)\n",
      "22/09/08 11:08:37 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 434.1 MiB)\n",
      "22/09/08 11:08:37 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on 192.168.1.81:35791 (size: 15.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:08:37 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 105 (MapPartitionsRDD[358] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:08:37 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:08:37 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 110) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:08:37 INFO Executor: Running task 0.0 in stage 105.0 (TID 110)\n",
      "22/09/08 11:08:37 INFO BlockManagerInfo: Removed broadcast_82_piece0 on 192.168.1.81:35791 in memory (size: 7.4 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:08:37 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:08:37 INFO Executor: Finished task 0.0 in stage 105.0 (TID 110). 2453 bytes result sent to driver\n",
      "22/09/08 11:08:37 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 110) in 33 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:08:37 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:08:37 INFO DAGScheduler: ShuffleMapStage 105 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.045 s\n",
      "22/09/08 11:08:37 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/08 11:08:37 INFO DAGScheduler: running: Set()\n",
      "22/09/08 11:08:37 INFO DAGScheduler: waiting: Set()\n",
      "22/09/08 11:08:37 INFO DAGScheduler: failed: Set()\n",
      "22/09/08 11:08:37 INFO ShufflePartitionsUtil: For shuffle(31), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/08 11:08:37 INFO BlockManagerInfo: Removed broadcast_76_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:08:37 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "22/09/08 11:08:37 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Got job 87 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Final stage: ResultStage 107 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 106)\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Submitting ResultStage 107 (MapPartitionsRDD[361] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:08:37 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 34.2 KiB, free 434.1 MiB)\n",
      "22/09/08 11:08:37 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 434.1 MiB)\n",
      "22/09/08 11:08:37 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on 192.168.1.81:35791 (size: 16.3 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:08:37 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 107 (MapPartitionsRDD[361] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:08:37 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:08:37 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 111) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:08:37 INFO Executor: Running task 0.0 in stage 107.0 (TID 111)\n",
      "22/09/08 11:08:37 INFO ShuffleBlockFetcherIterator: Getting 1 (216.0 B) non-empty blocks including 1 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:08:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:08:37 INFO Executor: Finished task 0.0 in stage 107.0 (TID 111). 3603 bytes result sent to driver\n",
      "22/09/08 11:08:37 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 111) in 11 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:08:37 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:08:37 INFO DAGScheduler: ResultStage 107 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.018 s\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Job 87 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:08:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 107: Stage finished\n",
      "22/09/08 11:08:37 INFO DAGScheduler: Job 87 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.033230 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-------+-----+\n|  level|count|\n+-------+-----+\n|   INFO| 1000|\n|  ERROR| 1000|\n|WARNING| 1000|\n+-------+-----+",
      "text/html": "<table border='1'>\n<tr><th>level</th><th>count</th></tr>\n<tr><td>INFO</td><td>1000</td></tr>\n<tr><td>ERROR</td><td>1000</td></tr>\n<tr><td>WARNING</td><td>1000</td></tr>\n</table>\n"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:08:37 INFO BlockManagerInfo: Removed broadcast_84_piece0 on 192.168.1.81:35791 in memory (size: 15.5 KiB, free: 434.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.groupBy(F.col(\"level\")).count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:09:06 INFO CodeGenerator: Code generated in 12.454519 ms\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Registering RDD 373 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 33\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Got map stage job 91 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Final stage: ShuffleMapStage 112 (count at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Submitting ShuffleMapStage 112 (MapPartitionsRDD[373] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:09:06 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 13.4 KiB, free 434.2 MiB)\n",
      "22/09/08 11:09:06 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 434.2 MiB)\n",
      "22/09/08 11:09:06 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on 192.168.1.81:35791 (size: 7.0 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:09:06 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 112 (MapPartitionsRDD[373] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:09:06 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:09:06 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 115) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4288 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:09:06 INFO Executor: Running task 0.0 in stage 112.0 (TID 115)\n",
      "22/09/08 11:09:06 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:09:06 INFO Executor: Finished task 0.0 in stage 112.0 (TID 115). 1881 bytes result sent to driver\n",
      "22/09/08 11:09:06 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 115) in 27 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:09:06 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:09:06 INFO DAGScheduler: ShuffleMapStage 112 (count at NativeMethodAccessorImpl.java:0) finished in 0.034 s\n",
      "22/09/08 11:09:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/08 11:09:06 INFO DAGScheduler: running: Set()\n",
      "22/09/08 11:09:06 INFO DAGScheduler: waiting: Set()\n",
      "22/09/08 11:09:06 INFO DAGScheduler: failed: Set()\n",
      "22/09/08 11:09:06 INFO CodeGenerator: Code generated in 7.375232 ms\n",
      "22/09/08 11:09:06 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Got job 92 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Final stage: ResultStage 114 (count at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 113)\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Submitting ResultStage 114 (MapPartitionsRDD[376] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:09:06 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 11.1 KiB, free 434.2 MiB)\n",
      "22/09/08 11:09:06 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.2 MiB)\n",
      "22/09/08 11:09:06 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on 192.168.1.81:35791 (size: 5.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:09:06 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 114 (MapPartitionsRDD[376] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:09:06 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:09:06 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 116) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:09:06 INFO Executor: Running task 0.0 in stage 114.0 (TID 116)\n",
      "22/09/08 11:09:06 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:09:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:09:06 INFO Executor: Finished task 0.0 in stage 114.0 (TID 116). 2656 bytes result sent to driver\n",
      "22/09/08 11:09:06 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 116) in 7 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:09:06 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:09:06 INFO DAGScheduler: ResultStage 114 (count at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:09:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 114: Stage finished\n",
      "22/09/08 11:09:06 INFO DAGScheduler: Job 92 finished: count at NativeMethodAccessorImpl.java:0, took 0.011896 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(F.col(\"level\") == \"ERROR\").count()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dmitryrusack/PycharmProjects/SparkArticle/venv/lib/python3.8/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable(\"temp_table\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:09:44 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Got job 99 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Final stage: ResultStage 123 (showString at <unknown>:0)\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[397] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:09:44 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 12.6 KiB, free 434.0 MiB)\n",
      "22/09/08 11:09:44 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.0 MiB)\n",
      "22/09/08 11:09:44 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:09:44 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 123 (MapPartitionsRDD[397] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:09:44 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:09:44 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 123) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:09:44 INFO Executor: Running task 0.0 in stage 123.0 (TID 123)\n",
      "22/09/08 11:09:44 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:09:44 INFO Executor: Finished task 0.0 in stage 123.0 (TID 123). 1784 bytes result sent to driver\n",
      "22/09/08 11:09:44 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 123) in 26 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:09:44 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:09:44 INFO DAGScheduler: ResultStage 123 (showString at <unknown>:0) finished in 0.030 s\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:09:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished\n",
      "22/09/08 11:09:44 INFO DAGScheduler: Job 99 finished: showString at <unknown>:0, took 0.033001 s\n",
      "+-----+-----+-------------------+-------+\n",
      "|id_pk|level|        update_time|   data|\n",
      "+-----+-----+-------------------+-------+\n",
      "|    1|ERROR|1970-01-01 10:50:40| Data 1|\n",
      "|    2|ERROR|1970-01-01 10:50:40| Data 2|\n",
      "|    3|ERROR|1970-01-01 10:50:40| Data 3|\n",
      "|    4|ERROR|1970-01-01 10:50:40| Data 4|\n",
      "|    5|ERROR|1970-01-01 10:50:40| Data 5|\n",
      "|    6|ERROR|1970-01-01 10:50:40| Data 6|\n",
      "|    7|ERROR|1970-01-01 10:50:40| Data 7|\n",
      "|    8|ERROR|1970-01-01 10:50:40| Data 8|\n",
      "|    9|ERROR|1970-01-01 10:50:40| Data 9|\n",
      "|   10|ERROR|1970-01-01 10:50:40|Data 10|\n",
      "|   11|ERROR|1970-01-01 10:50:40|Data 11|\n",
      "|   12|ERROR|1970-01-01 10:50:40|Data 12|\n",
      "|   13|ERROR|1970-01-01 10:50:40|Data 13|\n",
      "|   14|ERROR|1970-01-01 10:50:40|Data 14|\n",
      "|   15|ERROR|1970-01-01 10:50:40|Data 15|\n",
      "|   16|ERROR|1970-01-01 10:50:40|Data 16|\n",
      "|   17|ERROR|1970-01-01 10:50:40|Data 17|\n",
      "|   18|ERROR|1970-01-01 10:50:40|Data 18|\n",
      "|   19|ERROR|1970-01-01 10:50:40|Data 19|\n",
      "|   20|ERROR|1970-01-01 10:50:40|Data 20|\n",
      "+-----+-----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql(\"SELECT * FROM temp_table WHERE level='ERROR'\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:10:17 INFO CodeGenerator: Code generated in 18.172479 ms\n",
      "22/09/08 11:10:17 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Got job 103 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Final stage: ResultStage 128 (showString at <unknown>:0)\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Submitting ResultStage 128 (MapPartitionsRDD[409] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:10:17 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 12.7 KiB, free 434.4 MiB)\n",
      "22/09/08 11:10:17 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.4 MiB)\n",
      "22/09/08 11:10:17 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "22/09/08 11:10:17 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 128 (MapPartitionsRDD[409] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:10:17 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:10:17 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 127) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:17 INFO Executor: Running task 0.0 in stage 128.0 (TID 127)\n",
      "22/09/08 11:10:17 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:10:17 INFO Executor: Finished task 0.0 in stage 128.0 (TID 127). 1791 bytes result sent to driver\n",
      "22/09/08 11:10:17 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 127) in 26 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:10:17 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:17 INFO DAGScheduler: ResultStage 128 (showString at <unknown>:0) finished in 0.029 s\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 128: Stage finished\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Job 103 finished: showString at <unknown>:0, took 0.032679 s\n",
      "22/09/08 11:10:17 INFO CodeGenerator: Code generated in 13.447614 ms\n",
      "22/09/08 11:10:17 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Got job 104 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Final stage: ResultStage 129 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Submitting ResultStage 129 (MapPartitionsRDD[412] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:10:17 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 12.7 KiB, free 434.4 MiB)\n",
      "22/09/08 11:10:17 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.3 MiB)\n",
      "22/09/08 11:10:17 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "22/09/08 11:10:17 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 129 (MapPartitionsRDD[412] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:10:17 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:10:17 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 128) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4299 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:17 INFO Executor: Running task 0.0 in stage 129.0 (TID 128)\n",
      "22/09/08 11:10:17 INFO JDBCRDD: closed connection\n",
      "22/09/08 11:10:17 INFO Executor: Finished task 0.0 in stage 129.0 (TID 128). 1791 bytes result sent to driver\n",
      "22/09/08 11:10:17 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 128) in 17 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:10:17 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:17 INFO DAGScheduler: ResultStage 129 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.020 s\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Job 104 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 129: Stage finished\n",
      "22/09/08 11:10:17 INFO DAGScheduler: Job 104 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.021296 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-----+-----+-------------------+-------+-------+\n|id_pk|level|        update_time|   data|new_col|\n+-----+-----+-------------------+-------+-------+\n|    1|ERROR|1970-01-01 10:50:40| Data 1|   null|\n|    2|ERROR|1970-01-01 10:50:40| Data 2|   null|\n|    3|ERROR|1970-01-01 10:50:40| Data 3|   null|\n|    4|ERROR|1970-01-01 10:50:40| Data 4|   null|\n|    5|ERROR|1970-01-01 10:50:40| Data 5|   null|\n|    6|ERROR|1970-01-01 10:50:40| Data 6|   null|\n|    7|ERROR|1970-01-01 10:50:40| Data 7|   null|\n|    8|ERROR|1970-01-01 10:50:40| Data 8|   null|\n|    9|ERROR|1970-01-01 10:50:40| Data 9|   null|\n|   10|ERROR|1970-01-01 10:50:40|Data 10|   null|\n|   11|ERROR|1970-01-01 10:50:40|Data 11|   null|\n|   12|ERROR|1970-01-01 10:50:40|Data 12|   null|\n|   13|ERROR|1970-01-01 10:50:40|Data 13|   null|\n|   14|ERROR|1970-01-01 10:50:40|Data 14|   null|\n|   15|ERROR|1970-01-01 10:50:40|Data 15|   null|\n|   16|ERROR|1970-01-01 10:50:40|Data 16|   null|\n|   17|ERROR|1970-01-01 10:50:40|Data 17|   null|\n|   18|ERROR|1970-01-01 10:50:40|Data 18|   null|\n|   19|ERROR|1970-01-01 10:50:40|Data 19|   null|\n|   20|ERROR|1970-01-01 10:50:40|Data 20|   null|\n+-----+-----+-------------------+-------+-------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>id_pk</th><th>level</th><th>update_time</th><th>data</th><th>new_col</th></tr>\n<tr><td>1</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 1</td><td>null</td></tr>\n<tr><td>2</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 2</td><td>null</td></tr>\n<tr><td>3</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 3</td><td>null</td></tr>\n<tr><td>4</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 4</td><td>null</td></tr>\n<tr><td>5</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 5</td><td>null</td></tr>\n<tr><td>6</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 6</td><td>null</td></tr>\n<tr><td>7</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 7</td><td>null</td></tr>\n<tr><td>8</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 8</td><td>null</td></tr>\n<tr><td>9</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 9</td><td>null</td></tr>\n<tr><td>10</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 10</td><td>null</td></tr>\n<tr><td>11</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 11</td><td>null</td></tr>\n<tr><td>12</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 12</td><td>null</td></tr>\n<tr><td>13</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 13</td><td>null</td></tr>\n<tr><td>14</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 14</td><td>null</td></tr>\n<tr><td>15</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 15</td><td>null</td></tr>\n<tr><td>16</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 16</td><td>null</td></tr>\n<tr><td>17</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 17</td><td>null</td></tr>\n<tr><td>18</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 18</td><td>null</td></tr>\n<tr><td>19</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 19</td><td>null</td></tr>\n<tr><td>20</td><td>ERROR</td><td>1970-01-01 10:50:40</td><td>Data 20</td><td>null</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"new_col\", F.lit(None))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:10:31 INFO CodeGenerator: Code generated in 20.856444 ms\n",
      "22/09/08 11:10:31 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Got job 108 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Final stage: ResultStage 134 (showString at <unknown>:0)\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Submitting ResultStage 134 (MapPartitionsRDD[428] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:10:31 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 12.0 KiB, free 434.3 MiB)\n",
      "22/09/08 11:10:31 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.3 MiB)\n",
      "22/09/08 11:10:31 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on 192.168.1.81:35791 (size: 6.4 KiB, free: 434.4 MiB)\n",
      "22/09/08 11:10:31 INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 134 (MapPartitionsRDD[428] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:10:31 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:10:31 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 132) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:31 INFO Executor: Running task 0.0 in stage 134.0 (TID 132)\n",
      "22/09/08 11:10:31 INFO Executor: Finished task 0.0 in stage 134.0 (TID 132). 1847 bytes result sent to driver\n",
      "22/09/08 11:10:31 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 132) in 32 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:10:31 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:31 INFO DAGScheduler: ResultStage 134 (showString at <unknown>:0) finished in 0.037 s\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 134: Stage finished\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Job 108 finished: showString at <unknown>:0, took 0.040164 s\n",
      "22/09/08 11:10:31 INFO CodeGenerator: Code generated in 5.845277 ms\n",
      "22/09/08 11:10:31 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Got job 109 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Final stage: ResultStage 135 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Submitting ResultStage 135 (MapPartitionsRDD[430] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:10:31 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 12.0 KiB, free 434.3 MiB)\n",
      "22/09/08 11:10:31 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.3 MiB)\n",
      "22/09/08 11:10:31 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on 192.168.1.81:35791 (size: 6.4 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:10:31 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 135 (MapPartitionsRDD[430] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:10:31 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:10:31 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 133) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:31 INFO Executor: Running task 0.0 in stage 135.0 (TID 133)\n",
      "22/09/08 11:10:32 INFO Executor: Finished task 0.0 in stage 135.0 (TID 133). 1847 bytes result sent to driver\n",
      "22/09/08 11:10:32 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 133) in 15 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:10:32 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:32 INFO DAGScheduler: ResultStage 135 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.018 s\n",
      "22/09/08 11:10:32 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 135: Stage finished\n",
      "22/09/08 11:10:32 INFO DAGScheduler: Job 109 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.020373 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-------+\n|Celsius|\n+-------+\n|   -100|\n|    -99|\n|    -98|\n|    -97|\n|    -96|\n|    -95|\n|    -94|\n|    -93|\n|    -92|\n|    -91|\n|    -90|\n|    -89|\n|    -88|\n|    -87|\n|    -86|\n|    -85|\n|    -84|\n|    -83|\n|    -82|\n|    -81|\n+-------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>Celsius</th></tr>\n<tr><td>-100</td></tr>\n<tr><td>-99</td></tr>\n<tr><td>-98</td></tr>\n<tr><td>-97</td></tr>\n<tr><td>-96</td></tr>\n<tr><td>-95</td></tr>\n<tr><td>-94</td></tr>\n<tr><td>-93</td></tr>\n<tr><td>-92</td></tr>\n<tr><td>-91</td></tr>\n<tr><td>-90</td></tr>\n<tr><td>-89</td></tr>\n<tr><td>-88</td></tr>\n<tr><td>-87</td></tr>\n<tr><td>-86</td></tr>\n<tr><td>-85</td></tr>\n<tr><td>-84</td></tr>\n<tr><td>-83</td></tr>\n<tr><td>-82</td></tr>\n<tr><td>-81</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "temp_celsius = [(item,) for item in range(-100, 100)]\n",
    "schema = StructType([\n",
    "    StructField(\"Celsius\", IntegerType())\n",
    "])\n",
    "newDF = session.createDataFrame(data=temp_celsius, schema=schema)\n",
    "newDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:10:45 INFO CodeGenerator: Code generated in 14.140219 ms\n",
      "22/09/08 11:10:45 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Got job 113 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Final stage: ResultStage 140 (showString at <unknown>:0)\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting ResultStage 140 (MapPartitionsRDD[441] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 13.2 KiB, free 434.2 MiB)\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.2 MiB)\n",
      "22/09/08 11:10:45 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on 192.168.1.81:35791 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:10:45 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 140 (MapPartitionsRDD[441] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Adding task set 140.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 137) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:45 INFO Executor: Running task 0.0 in stage 140.0 (TID 137)\n",
      "22/09/08 11:10:45 INFO PythonRunner: Times: total = 19, boot = 3, init = 16, finish = 0\n",
      "22/09/08 11:10:45 INFO Executor: Finished task 0.0 in stage 140.0 (TID 137). 1845 bytes result sent to driver\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 137) in 24 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:45 INFO DAGScheduler: ResultStage 140 (showString at <unknown>:0) finished in 0.036 s\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 113 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 140: Stage finished\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 113 finished: showString at <unknown>:0, took 0.040767 s\n",
      "22/09/08 11:10:45 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Got job 114 (showString at <unknown>:0) with 2 output partitions\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Final stage: ResultStage 141 (showString at <unknown>:0)\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting ResultStage 141 (MapPartitionsRDD[441] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 13.2 KiB, free 434.2 MiB)\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.2 MiB)\n",
      "22/09/08 11:10:45 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on 192.168.1.81:35791 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:10:45 INFO SparkContext: Created broadcast 114 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 141 (MapPartitionsRDD[441] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Adding task set 141.0 with 2 tasks resource profile 0\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 138) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4825 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Starting task 1.0 in stage 141.0 (TID 139) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4757 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:45 INFO Executor: Running task 0.0 in stage 141.0 (TID 138)\n",
      "22/09/08 11:10:45 INFO Executor: Running task 1.0 in stage 141.0 (TID 139)\n",
      "22/09/08 11:10:45 INFO Executor: Finished task 0.0 in stage 141.0 (TID 138). 2047 bytes result sent to driver\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 138) in 39 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/08 11:10:45 INFO Executor: Finished task 1.0 in stage 141.0 (TID 139). 2070 bytes result sent to driver\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Finished task 1.0 in stage 141.0 (TID 139) in 47 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:45 INFO DAGScheduler: ResultStage 141 (showString at <unknown>:0) finished in 0.051 s\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 114 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 141: Stage finished\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 114 finished: showString at <unknown>:0, took 0.052611 s\n",
      "22/09/08 11:10:45 INFO CodeGenerator: Code generated in 5.653233 ms\n",
      "22/09/08 11:10:45 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Got job 115 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Final stage: ResultStage 142 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting ResultStage 142 (MapPartitionsRDD[443] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 13.2 KiB, free 434.1 MiB)\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "22/09/08 11:10:45 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on 192.168.1.81:35791 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:10:45 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 142 (MapPartitionsRDD[443] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Adding task set 142.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 140) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4921 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:45 INFO Executor: Running task 0.0 in stage 142.0 (TID 140)\n",
      "22/09/08 11:10:45 INFO PythonRunner: Times: total = 23, boot = 5, init = 18, finish = 0\n",
      "22/09/08 11:10:45 INFO Executor: Finished task 0.0 in stage 142.0 (TID 140). 1845 bytes result sent to driver\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 140) in 44 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:45 INFO DAGScheduler: ResultStage 142 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.049 s\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 115 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 142: Stage finished\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 115 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.051499 s\n",
      "22/09/08 11:10:45 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Got job 116 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Final stage: ResultStage 143 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting ResultStage 143 (MapPartitionsRDD[443] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 13.2 KiB, free 434.1 MiB)\n",
      "22/09/08 11:10:45 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.1 MiB)\n",
      "22/09/08 11:10:45 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on 192.168.1.81:35791 (size: 6.8 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:10:45 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 143 (MapPartitionsRDD[443] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Adding task set 143.0 with 2 tasks resource profile 0\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 141) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4825 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Starting task 1.0 in stage 143.0 (TID 142) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4757 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:10:45 INFO Executor: Running task 1.0 in stage 143.0 (TID 142)\n",
      "22/09/08 11:10:45 INFO Executor: Running task 0.0 in stage 143.0 (TID 141)\n",
      "22/09/08 11:10:45 INFO Executor: Finished task 0.0 in stage 143.0 (TID 141). 2047 bytes result sent to driver\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 141) in 23 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/08 11:10:45 INFO Executor: Finished task 1.0 in stage 143.0 (TID 142). 2070 bytes result sent to driver\n",
      "22/09/08 11:10:45 INFO TaskSetManager: Finished task 1.0 in stage 143.0 (TID 142) in 25 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:10:45 INFO DAGScheduler: ResultStage 143 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.030 s\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 116 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:10:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 143: Stage finished\n",
      "22/09/08 11:10:45 INFO DAGScheduler: Job 116 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.034594 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+-------+------------------+\n|Celsius|         Farengate|\n+-------+------------------+\n|      0|              32.0|\n|      1|              33.8|\n|      2|              35.6|\n|      3|              37.4|\n|      4|              39.2|\n|      5|              41.0|\n|      6|              42.8|\n|      7|              44.6|\n|      8|              46.4|\n|      9|              48.2|\n|     10|              50.0|\n|     11|              51.8|\n|     12|              53.6|\n|     13|55.400000000000006|\n|     14|              57.2|\n|     15|              59.0|\n|     16|              60.8|\n|     17|              62.6|\n|     18|              64.4|\n|     19|              66.2|\n+-------+------------------+\nonly showing top 20 rows",
      "text/html": "<table border='1'>\n<tr><th>Celsius</th><th>Farengate</th></tr>\n<tr><td>0</td><td>32.0</td></tr>\n<tr><td>1</td><td>33.8</td></tr>\n<tr><td>2</td><td>35.6</td></tr>\n<tr><td>3</td><td>37.4</td></tr>\n<tr><td>4</td><td>39.2</td></tr>\n<tr><td>5</td><td>41.0</td></tr>\n<tr><td>6</td><td>42.8</td></tr>\n<tr><td>7</td><td>44.6</td></tr>\n<tr><td>8</td><td>46.4</td></tr>\n<tr><td>9</td><td>48.2</td></tr>\n<tr><td>10</td><td>50.0</td></tr>\n<tr><td>11</td><td>51.8</td></tr>\n<tr><td>12</td><td>53.6</td></tr>\n<tr><td>13</td><td>55.400000000000006</td></tr>\n<tr><td>14</td><td>57.2</td></tr>\n<tr><td>15</td><td>59.0</td></tr>\n<tr><td>16</td><td>60.8</td></tr>\n<tr><td>17</td><td>62.6</td></tr>\n<tr><td>18</td><td>64.4</td></tr>\n<tr><td>19</td><td>66.2</td></tr>\n</table>\nonly showing top 20 rows\n"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newDF.withColumn(\"Farengate\", F.col(\"Celsius\") * (9 / 5) + 32).filter(F.col(\"Celsius\") >= 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "people = [(\"Jon\", 1,), (\"Mark\", 2), (\"Oleg\", 1)]\n",
    "column_people = [\"name\", \"departament\"]\n",
    "departaments = [(1, \"Dep_1\"), (2, \"Dep_2\")]\n",
    "columns_departaments = [\"id\", \"name_dep\"]\n",
    "peopleDF = session.createDataFrame(people, schema=column_people)\n",
    "departDF = session.createDataFrame(departaments, schema=columns_departaments)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:11:11 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Got job 127 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Final stage: ResultStage 156 (showString at <unknown>:0)\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting ResultStage 156 (MapPartitionsRDD[477] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 12.4 KiB, free 434.3 MiB)\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.3 MiB)\n",
      "22/09/08 11:11:11 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:11 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 156 (MapPartitionsRDD[477] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Adding task set 156.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 155) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4468 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:11 INFO Executor: Running task 0.0 in stage 156.0 (TID 155)\n",
      "22/09/08 11:11:11 INFO PythonRunner: Times: total = 2, boot = -144, init = 146, finish = 0\n",
      "22/09/08 11:11:11 INFO Executor: Finished task 0.0 in stage 156.0 (TID 155). 1832 bytes result sent to driver\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 155) in 24 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:11 INFO DAGScheduler: ResultStage 156 (showString at <unknown>:0) finished in 0.028 s\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 127 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 156: Stage finished\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 127 finished: showString at <unknown>:0, took 0.030014 s\n",
      "22/09/08 11:11:11 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Got job 128 (showString at <unknown>:0) with 2 output partitions\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Final stage: ResultStage 157 (showString at <unknown>:0)\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting ResultStage 157 (MapPartitionsRDD[477] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 12.4 KiB, free 434.2 MiB)\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.2 MiB)\n",
      "22/09/08 11:11:11 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:11 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 157 (MapPartitionsRDD[477] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Adding task set 157.0 with 2 tasks resource profile 0\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 156) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Starting task 1.0 in stage 157.0 (TID 157) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:11 INFO Executor: Running task 1.0 in stage 157.0 (TID 157)\n",
      "22/09/08 11:11:11 INFO Executor: Running task 0.0 in stage 157.0 (TID 156)\n",
      "22/09/08 11:11:11 INFO PythonRunner: Times: total = 5, boot = -112, init = 117, finish = 0\n",
      "22/09/08 11:11:11 INFO Executor: Finished task 1.0 in stage 157.0 (TID 157). 1833 bytes result sent to driver\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Finished task 1.0 in stage 157.0 (TID 157) in 24 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/08 11:11:11 INFO PythonRunner: Times: total = 66, boot = -23, init = 89, finish = 0\n",
      "22/09/08 11:11:11 INFO Executor: Finished task 0.0 in stage 157.0 (TID 156). 1833 bytes result sent to driver\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 156) in 72 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:11 INFO DAGScheduler: ResultStage 157 (showString at <unknown>:0) finished in 0.078 s\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 128 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 157: Stage finished\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 128 finished: showString at <unknown>:0, took 0.079721 s\n",
      "22/09/08 11:11:11 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Got job 129 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Final stage: ResultStage 158 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting ResultStage 158 (MapPartitionsRDD[479] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 12.4 KiB, free 434.2 MiB)\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.2 MiB)\n",
      "22/09/08 11:11:11 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:11 INFO SparkContext: Created broadcast 129 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 158 (MapPartitionsRDD[479] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Adding task set 158.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 158) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4468 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:11 INFO Executor: Running task 0.0 in stage 158.0 (TID 158)\n",
      "22/09/08 11:11:11 INFO PythonRunner: Times: total = 11, boot = -94, init = 105, finish = 0\n",
      "22/09/08 11:11:11 INFO Executor: Finished task 0.0 in stage 158.0 (TID 158). 1832 bytes result sent to driver\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 158) in 27 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:11 INFO DAGScheduler: ResultStage 158 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.031 s\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 129 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 158: Stage finished\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 129 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.032091 s\n",
      "22/09/08 11:11:11 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Got job 130 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Final stage: ResultStage 159 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting ResultStage 159 (MapPartitionsRDD[479] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 12.4 KiB, free 434.2 MiB)\n",
      "22/09/08 11:11:11 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.2 MiB)\n",
      "22/09/08 11:11:11 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:11 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 159 (MapPartitionsRDD[479] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Adding task set 159.0 with 2 tasks resource profile 0\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 159) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Starting task 1.0 in stage 159.0 (TID 160) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4469 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:11 INFO Executor: Running task 1.0 in stage 159.0 (TID 160)\n",
      "22/09/08 11:11:11 INFO Executor: Running task 0.0 in stage 159.0 (TID 159)\n",
      "22/09/08 11:11:11 INFO PythonRunner: Times: total = 2, boot = -66, init = 68, finish = 0\n",
      "22/09/08 11:11:11 INFO Executor: Finished task 0.0 in stage 159.0 (TID 159). 1833 bytes result sent to driver\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 159) in 22 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/08 11:11:11 INFO PythonRunner: Times: total = 49, boot = -16, init = 64, finish = 1\n",
      "22/09/08 11:11:11 INFO Executor: Finished task 1.0 in stage 159.0 (TID 160). 1833 bytes result sent to driver\n",
      "22/09/08 11:11:11 INFO TaskSetManager: Finished task 1.0 in stage 159.0 (TID 160) in 57 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:11 INFO DAGScheduler: ResultStage 159 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.062 s\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 130 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 159: Stage finished\n",
      "22/09/08 11:11:11 INFO DAGScheduler: Job 130 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.063105 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+----+-----------+\n|name|departament|\n+----+-----------+\n| Jon|          1|\n|Mark|          2|\n|Oleg|          1|\n+----+-----------+",
      "text/html": "<table border='1'>\n<tr><th>name</th><th>departament</th></tr>\n<tr><td>Jon</td><td>1</td></tr>\n<tr><td>Mark</td><td>2</td></tr>\n<tr><td>Oleg</td><td>1</td></tr>\n</table>\n"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:11:14 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:11:14 INFO DAGScheduler: Got job 136 (showString at <unknown>:0) with 1 output partitions\n",
      "22/09/08 11:11:14 INFO DAGScheduler: Final stage: ResultStage 166 (showString at <unknown>:0)\n",
      "22/09/08 11:11:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:14 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:14 INFO DAGScheduler: Submitting ResultStage 166 (MapPartitionsRDD[492] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:11:14 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/08 11:11:14 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/08 11:11:14 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:14 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 166 (MapPartitionsRDD[492] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:11:14 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:11:14 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 167) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:14 INFO Executor: Running task 0.0 in stage 166.0 (TID 167)\n",
      "22/09/08 11:11:15 INFO PythonRunner: Times: total = 11, boot = -2768, init = 2779, finish = 0\n",
      "22/09/08 11:11:15 INFO Executor: Finished task 0.0 in stage 166.0 (TID 167). 1789 bytes result sent to driver\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 167) in 22 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:15 INFO DAGScheduler: ResultStage 166 (showString at <unknown>:0) finished in 0.027 s\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 136 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 166: Stage finished\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 136 finished: showString at <unknown>:0, took 0.029056 s\n",
      "22/09/08 11:11:15 INFO SparkContext: Starting job: showString at <unknown>:0\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Got job 137 (showString at <unknown>:0) with 2 output partitions\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Final stage: ResultStage 167 (showString at <unknown>:0)\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Submitting ResultStage 167 (MapPartitionsRDD[492] at showString at <unknown>:0), which has no missing parents\n",
      "22/09/08 11:11:15 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/08 11:11:15 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.1 MiB)\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 167 (MapPartitionsRDD[492] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Adding task set 167.0 with 2 tasks resource profile 0\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 168) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Starting task 1.0 in stage 167.0 (TID 169) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:15 INFO Executor: Running task 1.0 in stage 167.0 (TID 169)\n",
      "22/09/08 11:11:15 INFO Executor: Running task 0.0 in stage 167.0 (TID 168)\n",
      "22/09/08 11:11:15 INFO PythonRunner: Times: total = 3, boot = -2748, init = 2751, finish = 0\n",
      "22/09/08 11:11:15 INFO Executor: Finished task 0.0 in stage 167.0 (TID 168). 1830 bytes result sent to driver\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 168) in 14 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/08 11:11:15 INFO PythonRunner: Times: total = 45, boot = -1, init = 45, finish = 1\n",
      "22/09/08 11:11:15 INFO Executor: Finished task 1.0 in stage 167.0 (TID 169). 1830 bytes result sent to driver\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Finished task 1.0 in stage 167.0 (TID 169) in 48 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:15 INFO DAGScheduler: ResultStage 167 (showString at <unknown>:0) finished in 0.052 s\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 137 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 167: Stage finished\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 137 finished: showString at <unknown>:0, took 0.054584 s\n",
      "22/09/08 11:11:15 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Got job 138 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Final stage: ResultStage 168 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Submitting ResultStage 168 (MapPartitionsRDD[494] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:11:15 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 12.4 KiB, free 434.1 MiB)\n",
      "22/09/08 11:11:15 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.0 MiB)\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 168 (MapPartitionsRDD[494] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 170) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:15 INFO Executor: Running task 0.0 in stage 168.0 (TID 170)\n",
      "22/09/08 11:11:15 INFO PythonRunner: Times: total = 1, boot = -47, init = 48, finish = 0\n",
      "22/09/08 11:11:15 INFO Executor: Finished task 0.0 in stage 168.0 (TID 170). 1789 bytes result sent to driver\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 170) in 12 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:15 INFO DAGScheduler: ResultStage 168 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.016 s\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 138 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 168: Stage finished\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 138 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.017154 s\n",
      "22/09/08 11:11:15 INFO SparkContext: Starting job: getRowsToPython at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Got job 139 (getRowsToPython at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Final stage: ResultStage 169 (getRowsToPython at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Submitting ResultStage 169 (MapPartitionsRDD[494] at getRowsToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:11:15 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 12.4 KiB, free 434.0 MiB)\n",
      "22/09/08 11:11:15 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.0 MiB)\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on 192.168.1.81:35791 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 169 (MapPartitionsRDD[494] at getRowsToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2))\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Adding task set 169.0 with 2 tasks resource profile 0\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Removed broadcast_135_piece0 on 192.168.1.81:35791 in memory (size: 5.9 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 171) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Starting task 1.0 in stage 169.0 (TID 172) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4470 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:11:15 INFO Executor: Running task 1.0 in stage 169.0 (TID 172)\n",
      "22/09/08 11:11:15 INFO Executor: Running task 0.0 in stage 169.0 (TID 171)\n",
      "22/09/08 11:11:15 INFO PythonRunner: Times: total = 22, boot = -57, init = 79, finish = 0\n",
      "22/09/08 11:11:15 INFO Executor: Finished task 0.0 in stage 169.0 (TID 171). 1830 bytes result sent to driver\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 171) in 34 ms on 192.168.1.81 (executor driver) (1/2)\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Removed broadcast_123_piece0 on 192.168.1.81:35791 in memory (size: 7.4 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Removed broadcast_121_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO PythonRunner: Times: total = 73, boot = -32, init = 105, finish = 0\n",
      "22/09/08 11:11:15 INFO Executor: Finished task 1.0 in stage 169.0 (TID 172). 1830 bytes result sent to driver\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Removed broadcast_125_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO BlockManagerInfo: Removed broadcast_138_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:11:15 INFO TaskSetManager: Finished task 1.0 in stage 169.0 (TID 172) in 135 ms on 192.168.1.81 (executor driver) (2/2)\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:11:15 INFO DAGScheduler: ResultStage 169 (getRowsToPython at NativeMethodAccessorImpl.java:0) finished in 0.163 s\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 139 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:11:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 169: Stage finished\n",
      "22/09/08 11:11:15 INFO DAGScheduler: Job 139 finished: getRowsToPython at NativeMethodAccessorImpl.java:0, took 0.164598 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "+---+--------+\n| id|name_dep|\n+---+--------+\n|  1|   Dep_1|\n|  2|   Dep_2|\n+---+--------+",
      "text/html": "<table border='1'>\n<tr><th>id</th><th>name_dep</th></tr>\n<tr><td>1</td><td>Dep_1</td></tr>\n<tr><td>2</td><td>Dep_2</td></tr>\n</table>\n"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "departDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "joindf = peopleDF.join(departDF, peopleDF[\"departament\"] == departDF[\"id\"]).select(F.col(\"name\"), F.col(\"name_dep\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:12:41 INFO DAGScheduler: Registering RDD 589 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 55\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Got map stage job 168 (save at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Final stage: ShuffleMapStage 211 (save at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Submitting ShuffleMapStage 211 (MapPartitionsRDD[589] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:12:41 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 15.2 KiB, free 434.0 MiB)\n",
      "22/09/08 11:12:41 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.0 MiB)\n",
      "22/09/08 11:12:41 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on 192.168.1.81:35791 (size: 8.0 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:12:41 INFO SparkContext: Created broadcast 168 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 211 (MapPartitionsRDD[589] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Adding task set 211.0 with 3 tasks resource profile 0\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 0.0 in stage 211.0 (TID 219) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4457 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 1.0 in stage 211.0 (TID 220) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4458 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 2.0 in stage 211.0 (TID 221) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4458 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO Executor: Running task 0.0 in stage 211.0 (TID 219)\n",
      "22/09/08 11:12:41 INFO Executor: Running task 1.0 in stage 211.0 (TID 220)\n",
      "22/09/08 11:12:41 INFO Executor: Running task 2.0 in stage 211.0 (TID 221)\n",
      "22/09/08 11:12:41 INFO PythonRunner: Times: total = 12, boot = -9081, init = 9093, finish = 0\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 2.0 in stage 211.0 (TID 221). 2389 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 2.0 in stage 211.0 (TID 221) in 25 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/08 11:12:41 INFO PythonRunner: Times: total = 20, boot = -9020, init = 9040, finish = 0\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Registering RDD 591 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 56\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Got map stage job 169 (save at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Final stage: ShuffleMapStage 212 (save at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 0.0 in stage 211.0 (TID 219). 2389 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Submitting ShuffleMapStage 212 (MapPartitionsRDD[591] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 0.0 in stage 211.0 (TID 219) in 43 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/08 11:12:41 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 15.2 KiB, free 434.0 MiB)\n",
      "22/09/08 11:12:41 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 434.0 MiB)\n",
      "22/09/08 11:12:41 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on 192.168.1.81:35791 (size: 8.0 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:12:41 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 212 (MapPartitionsRDD[591] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Adding task set 212.0 with 3 tasks resource profile 0\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 0.0 in stage 212.0 (TID 222) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4422 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 1.0 in stage 212.0 (TID 223) (192.168.1.81, executor driver, partition 1, PROCESS_LOCAL, 4459 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO Executor: Running task 0.0 in stage 212.0 (TID 222)\n",
      "22/09/08 11:12:41 INFO Executor: Running task 1.0 in stage 212.0 (TID 223)\n",
      "22/09/08 11:12:41 INFO PythonRunner: Times: total = 44, boot = -9090, init = 9133, finish = 1\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 1.0 in stage 211.0 (TID 220). 2389 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 2.0 in stage 212.0 (TID 224) (192.168.1.81, executor driver, partition 2, PROCESS_LOCAL, 4459 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 1.0 in stage 211.0 (TID 220) in 62 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Removed TaskSet 211.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:12:41 INFO DAGScheduler: ShuffleMapStage 211 (save at NativeMethodAccessorImpl.java:0) finished in 0.068 s\n",
      "22/09/08 11:12:41 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/08 11:12:41 INFO DAGScheduler: running: Set(ShuffleMapStage 212)\n",
      "22/09/08 11:12:41 INFO DAGScheduler: waiting: Set()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: failed: Set()\n",
      "22/09/08 11:12:41 INFO Executor: Running task 2.0 in stage 212.0 (TID 224)\n",
      "22/09/08 11:12:41 INFO PythonRunner: Times: total = 16, boot = 1, init = 15, finish = 0\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 0.0 in stage 212.0 (TID 222). 2260 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 0.0 in stage 212.0 (TID 222) in 55 ms on 192.168.1.81 (executor driver) (1/3)\n",
      "22/09/08 11:12:41 INFO PythonRunner: Times: total = 3, boot = -27, init = 30, finish = 0\n",
      "22/09/08 11:12:41 INFO PythonRunner: Times: total = 55, boot = -21, init = 76, finish = 0\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 2.0 in stage 212.0 (TID 224). 2389 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 2.0 in stage 212.0 (TID 224) in 50 ms on 192.168.1.81 (executor driver) (2/3)\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 1.0 in stage 212.0 (TID 223). 2389 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 1.0 in stage 212.0 (TID 223) in 67 ms on 192.168.1.81 (executor driver) (3/3)\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:12:41 INFO DAGScheduler: ShuffleMapStage 212 (save at NativeMethodAccessorImpl.java:0) finished in 0.071 s\n",
      "22/09/08 11:12:41 INFO DAGScheduler: looking for newly runnable stages\n",
      "22/09/08 11:12:41 INFO DAGScheduler: running: Set()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: waiting: Set()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: failed: Set()\n",
      "22/09/08 11:12:41 INFO ShufflePartitionsUtil: For shuffle(55, 56), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "22/09/08 11:12:41 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Got job 170 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Final stage: ResultStage 215 (save at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 213, ShuffleMapStage 214)\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Submitting ResultStage 215 (MapPartitionsRDD[600] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:12:41 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 47.3 KiB, free 434.0 MiB)\n",
      "22/09/08 11:12:41 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 21.4 KiB, free 433.9 MiB)\n",
      "22/09/08 11:12:41 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on 192.168.1.81:35791 (size: 21.4 KiB, free: 434.3 MiB)\n",
      "22/09/08 11:12:41 INFO SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 215 (MapPartitionsRDD[600] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Adding task set 215.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Starting task 0.0 in stage 215.0 (TID 225) (192.168.1.81, executor driver, partition 0, NODE_LOCAL, 4735 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:12:41 INFO Executor: Running task 0.0 in stage 215.0 (TID 225)\n",
      "22/09/08 11:12:41 INFO ShuffleBlockFetcherIterator: Getting 3 (216.0 B) non-empty blocks including 3 (216.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:12:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:12:41 INFO ShuffleBlockFetcherIterator: Getting 2 (144.0 B) non-empty blocks including 2 (144.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "22/09/08 11:12:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "22/09/08 11:12:41 INFO Executor: Finished task 0.0 in stage 215.0 (TID 225). 4411 bytes result sent to driver\n",
      "22/09/08 11:12:41 INFO TaskSetManager: Finished task 0.0 in stage 215.0 (TID 225) in 23 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Removed TaskSet 215.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:12:41 INFO DAGScheduler: ResultStage 215 (save at NativeMethodAccessorImpl.java:0) finished in 0.032 s\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Job 170 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:12:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 215: Stage finished\n",
      "22/09/08 11:12:41 INFO DAGScheduler: Job 170 finished: save at NativeMethodAccessorImpl.java:0, took 0.034833 s\n"
     ]
    }
   ],
   "source": [
    "(joindf.write\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\n",
    " .option(\"dbtable\", \"out_table\")\n",
    " .option(\"user\", \"user\")\n",
    " .option(\"password\", \"password\")\n",
    " .option(\"driver\", \"org.postgresql.Driver\")\n",
    " # .mode(\"overwrite\")\n",
    " .mode(\"append\")\n",
    " .save())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:14:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "22/09/08 11:14:13 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "22/09/08 11:14:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/09/08 11:14:13 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#1593, None)) > 0)\n",
      "22/09/08 11:14:13 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_205 stored as values in memory (estimated size 200.1 KiB, free 433.4 MiB)\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_205_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.4 MiB)\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on 192.168.1.81:35791 (size: 34.3 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:14:13 INFO SparkContext: Created broadcast 205 from csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:14:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/09/08 11:14:13 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Got job 201 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Final stage: ResultStage 260 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Submitting ResultStage 260 (MapPartitionsRDD[708] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_206 stored as values in memory (estimated size 11.8 KiB, free 433.3 MiB)\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_206_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 433.3 MiB)\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on 192.168.1.81:35791 (size: 5.9 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:14:13 INFO SparkContext: Created broadcast 206 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 260 (MapPartitionsRDD[708] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:14:13 INFO TaskSchedulerImpl: Adding task set 260.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:14:13 INFO TaskSetManager: Starting task 0.0 in stage 260.0 (TID 272) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4937 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:14:13 INFO Executor: Running task 0.0 in stage 260.0 (TID 272)\n",
      "22/09/08 11:14:13 INFO FileScanRDD: Reading File path: file:///home/dmitryrusack/Work/about_spark/practic_3/test_file.csv, range: 0-127679, partition values: [empty row]\n",
      "22/09/08 11:14:13 INFO Executor: Finished task 0.0 in stage 260.0 (TID 272). 1558 bytes result sent to driver\n",
      "22/09/08 11:14:13 INFO TaskSetManager: Finished task 0.0 in stage 260.0 (TID 272) in 7 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:14:13 INFO TaskSchedulerImpl: Removed TaskSet 260.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:14:13 INFO DAGScheduler: ResultStage 260 (csv at NativeMethodAccessorImpl.java:0) finished in 0.010 s\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Job 201 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:14:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 260: Stage finished\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Job 201 finished: csv at NativeMethodAccessorImpl.java:0, took 0.011627 s\n",
      "22/09/08 11:14:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "22/09/08 11:14:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "22/09/08 11:14:13 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_207 stored as values in memory (estimated size 200.1 KiB, free 433.1 MiB)\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_207_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.1 MiB)\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on 192.168.1.81:35791 (size: 34.3 KiB, free: 434.1 MiB)\n",
      "22/09/08 11:14:13 INFO SparkContext: Created broadcast 207 from csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:14:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "22/09/08 11:14:13 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Got job 202 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Final stage: ResultStage 261 (csv at NativeMethodAccessorImpl.java:0)\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Missing parents: List()\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Submitting ResultStage 261 (MapPartitionsRDD[714] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_208 stored as values in memory (estimated size 24.9 KiB, free 433.1 MiB)\n",
      "22/09/08 11:14:13 INFO MemoryStore: Block broadcast_208_piece0 stored as bytes in memory (estimated size 11.8 KiB, free 433.1 MiB)\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on 192.168.1.81:35791 (size: 11.8 KiB, free: 434.1 MiB)\n",
      "22/09/08 11:14:13 INFO SparkContext: Created broadcast 208 from broadcast at DAGScheduler.scala:1513\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Removed broadcast_203_piece0 on 192.168.1.81:35791 in memory (size: 7.4 KiB, free: 434.1 MiB)\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 261 (MapPartitionsRDD[714] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Removed broadcast_196_piece0 on 192.168.1.81:35791 in memory (size: 6.5 KiB, free: 434.1 MiB)\n",
      "22/09/08 11:14:13 INFO TaskSchedulerImpl: Adding task set 261.0 with 1 tasks resource profile 0\n",
      "22/09/08 11:14:13 INFO TaskSetManager: Starting task 0.0 in stage 261.0 (TID 273) (192.168.1.81, executor driver, partition 0, PROCESS_LOCAL, 4937 bytes) taskResourceAssignments Map()\n",
      "22/09/08 11:14:13 INFO Executor: Running task 0.0 in stage 261.0 (TID 273)\n",
      "22/09/08 11:14:13 INFO FileScanRDD: Reading File path: file:///home/dmitryrusack/Work/about_spark/practic_3/test_file.csv, range: 0-127679, partition values: [empty row]\n",
      "22/09/08 11:14:13 INFO BlockManagerInfo: Removed broadcast_199_piece0 on 192.168.1.81:35791 in memory (size: 5.9 KiB, free: 434.2 MiB)\n",
      "22/09/08 11:14:13 INFO Executor: Finished task 0.0 in stage 261.0 (TID 273). 1571 bytes result sent to driver\n",
      "22/09/08 11:14:13 INFO TaskSetManager: Finished task 0.0 in stage 261.0 (TID 273) in 30 ms on 192.168.1.81 (executor driver) (1/1)\n",
      "22/09/08 11:14:13 INFO TaskSchedulerImpl: Removed TaskSet 261.0, whose tasks have all completed, from pool \n",
      "22/09/08 11:14:13 INFO DAGScheduler: ResultStage 261 (csv at NativeMethodAccessorImpl.java:0) finished in 0.060 s\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Job 202 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "22/09/08 11:14:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 261: Stage finished\n",
      "22/09/08 11:14:13 INFO DAGScheduler: Job 202 finished: csv at NativeMethodAccessorImpl.java:0, took 0.062624 s\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType()),\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"message\", StringType())\n",
    "])\n",
    "# df_from_local_file = session.read.csv(\"./test_file.csv\", schema=schema)\n",
    "df_from_local_file = session.read.csv(\"./test_file.csv\", inferSchema=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: timestamp (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_local_file.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/08 11:14:16 INFO SparkUI: Stopped Spark web UI at http://192.168.1.81:4040\n",
      "22/09/08 11:14:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "22/09/08 11:14:16 INFO MemoryStore: MemoryStore cleared\n",
      "22/09/08 11:14:16 INFO BlockManager: BlockManager stopped\n",
      "22/09/08 11:14:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "22/09/08 11:14:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "22/09/08 11:14:16 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "session.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}